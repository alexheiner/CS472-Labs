{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Unsupervised Learning: Clustering Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from scipy.io import arff\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. (50%) Implement the k-means clustering algorithm and the HAC (Hierarchical Agglomerative Clustering) algorithm.\n",
        "\n",
        "### 1.1.1 HAC\n",
        "\n",
        "### Code requirements \n",
        "- HAC should support both single link and complete link options.\n",
        "- HAC automatically generates all clusterings from n to 1.  To simplify the amount of output you may want to implement a mechanism to specify for which k values actual output will be generated.\n",
        "\n",
        "\n",
        "---\n",
        "The output should include the following:\n",
        "- The number of clusters (k).\n",
        "- The silhouette score of the full clustering. (You can either write and use your own silhouette_score function (extra credit) or use sklearn's)\n",
        "\n",
        "\n",
        "For each cluster report include:\n",
        "\n",
        "\n",
        "- The centroid id.\n",
        "- The number of instances tied to that centroid. \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_a2KSZ_7AN0G"
      },
      "outputs": [],
      "source": [
        "class HACClustering(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self,k=3,link_type='single'): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            link_type = single or complete. when combining two clusters use complete link or single link\n",
        "        \"\"\"\n",
        "        self.link_type = link_type\n",
        "        self.k = k\n",
        "        self.clusters = []\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        cluster_arr = [np.asarray([item]) for item in X]\n",
        "        self.clusters = cluster_arr\n",
        "        is_done = False\n",
        "        while not is_done:\n",
        "          # print('clusters: ', len(self.clusters))\n",
        "          if len(self.clusters) == self.k:\n",
        "            is_done = True\n",
        "            break\n",
        "          if self.link_type == 'single':\n",
        "            self.clusters = self.single_cluster(self.clusters)\n",
        "          else:\n",
        "            self.clusters = self.cluster_complete_link(self.clusters)\n",
        "        return self\n",
        "\n",
        "    def score(self):\n",
        "      self.sse = []\n",
        "      self.total_sse = 0\n",
        "      self.centroids = []\n",
        "      # find centroids\n",
        "      for i in range(len(self.clusters)):\n",
        "        centroid = np.mean(self.clusters[i], axis=0)\n",
        "        self.centroids.append(centroid)\n",
        "        # find cluster sse\n",
        "        tmp = np.linalg.norm(self.clusters[i] - centroid)\n",
        "        squared_err = tmp**2\n",
        "        self.sse.append(np.sum(squared_err))\n",
        "        self.total_sse += squared_err\n",
        "\n",
        "    def single_cluster(self, clusters):\n",
        "      distances = []\n",
        "      cluster_indexes = []\n",
        "      for i in range(len(clusters)):\n",
        "        tmp_dist = []\n",
        "        for j in range(len(clusters)):\n",
        "          min_dist = np.inf\n",
        "          if i == j:\n",
        "            tmp_dist.append(np.inf)\n",
        "            continue\n",
        "          if len(clusters[i]) > 1:\n",
        "            # check if j is also a cluster\n",
        "            if len(clusters[j]) > 1:\n",
        "            # calc distance between the two clusters\n",
        "              for k in range(len(clusters[i])):\n",
        "                for l in range(len(clusters[j])):\n",
        "                  dist = np.linalg.norm(clusters[i][k] - clusters[j][l])\n",
        "                  if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "            else: \n",
        "              for k in range(len(clusters[i])):\n",
        "                dist = np.linalg.norm(clusters[i][k] - clusters[j])\n",
        "                if dist < min_dist:\n",
        "                  min_dist = dist\n",
        "          else: \n",
        "            if len(clusters[j]) > 1:\n",
        "            # i is single cluster, j has more than one\n",
        "              for k in range(len(clusters[j])):\n",
        "                dist = np.linalg.norm(clusters[i] - clusters[j][k])\n",
        "                if dist < min_dist:\n",
        "                  min_dist = dist\n",
        "            else:\n",
        "              min_dist = np.linalg.norm(clusters[i] - clusters[j])\n",
        "          tmp_dist.append(min_dist)\n",
        "        min_val = min(tmp_dist)\n",
        "        index = tmp_dist.index(min_val)\n",
        "        distances.append(min_val)\n",
        "        cluster_indexes.append(index)\n",
        "      \n",
        "      best_dist = min(distances)\n",
        "      dist_index = distances.index(best_dist)\n",
        "      prev_index = cluster_indexes[dist_index]\n",
        "      \n",
        "      comb_arr = clusters[prev_index]\n",
        "      comb_arr = np.append(clusters[dist_index], np.array(clusters[prev_index]), axis=0)\n",
        "      clusters[dist_index] = comb_arr\n",
        "      new_clusters = np.delete(clusters, prev_index)\n",
        "      # clusters[prev_index].pop()\n",
        "      return new_clusters\n",
        "\n",
        "    def cluster_complete_link(self, clusters):\n",
        "      distances = []\n",
        "      cluster_indexes = []\n",
        "      for i in range(len(clusters)):\n",
        "        tmp_dist = []\n",
        "        for j in range(len(clusters)):\n",
        "          min_dist = 0\n",
        "          if i == j:\n",
        "            tmp_dist.append(np.inf)\n",
        "            continue\n",
        "          if len(clusters[i]) > 1:\n",
        "            # check if j is also a cluster\n",
        "            if len(clusters[j]) > 1:\n",
        "            # calc distance between the two clusters\n",
        "              for k in range(len(clusters[i])):\n",
        "                for l in range(len(clusters[j])):\n",
        "                  dist = np.linalg.norm(clusters[i][k] - clusters[j][l]) \n",
        "                  if dist > min_dist:\n",
        "                    min_dist = dist\n",
        "            else: \n",
        "              for k in range(len(clusters[i])):\n",
        "                dist = np.linalg.norm(clusters[i][k] - clusters[j])\n",
        "                if dist > min_dist:\n",
        "                  min_dist = dist\n",
        "          else: \n",
        "            if len(clusters[j]) > 1:\n",
        "            # i is single cluster, j has more than one\n",
        "              for k in range(len(clusters[j])):\n",
        "                dist = np.linalg.norm(clusters[i] - clusters[j][k])\n",
        "                if dist > min_dist:\n",
        "                  min_dist = dist\n",
        "            else:\n",
        "              min_dist = np.linalg.norm(clusters[i] - clusters[j])\n",
        "          tmp_dist.append(min_dist)\n",
        "        min_val = min(tmp_dist)\n",
        "        index = tmp_dist.index(min(tmp_dist))\n",
        "        distances.append(min_val)\n",
        "        cluster_indexes.append(index)\n",
        "      \n",
        "      best_dist = min(distances)\n",
        "      dist_index = distances.index(best_dist)\n",
        "      prev_index = cluster_indexes[dist_index]\n",
        "      \n",
        "      comb_arr = clusters[prev_index]\n",
        "      comb_arr = np.append(clusters[dist_index], np.array(clusters[prev_index]), axis=0)\n",
        "      clusters[dist_index] = comb_arr\n",
        "      new_clusters = np.delete(clusters, prev_index)\n",
        "      # clusters[prev_index].pop()\n",
        "      return new_clusters\n",
        "    \n",
        "    def print_clusters(self):\n",
        "      \"\"\"\n",
        "        Used for grading.\n",
        "        print(\"Num clusters: {:d}\\n\".format(k))\n",
        "        print(\"Silhouette score: {:.4f}\\n\\n\".format(silhouette_score))\n",
        "        for each cluster and centroid:\n",
        "          print(np.array2string(centroid,precision=4,separator=\",\"))\n",
        "          print(\"{:d}\\n\".format(size of cluster))\n",
        "      \"\"\"\n",
        "      # print results\n",
        "      print(\"Num clusters: {:d}\".format(len(self.clusters)))\n",
        "      print(\"SSE score: {:.4f}\\n\\n\".format(self.total_sse))\n",
        "      for i in range(len(self.clusters)):\n",
        "        print(np.array2string(self.centroids[i],precision=4,separator=\",\"))\n",
        "        print(\"{:d}\".format(len(self.clusters[i])))\n",
        "        print(\"{:.4f}\\n\".format(self.sse[i]))\n",
        "      pass\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibCIXIThpbE"
      },
      "source": [
        "### 1.1.2 Debug \n",
        "\n",
        "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff)\n",
        "\n",
        "\n",
        "---\n",
        "The dataset was modified to be a lot smaller. The last datapoint should be on line 359 or the point 0.585,0.46,0.185,0.922,0.3635,0.213,0.285,10. The remaining points should be commented out.\n",
        "\n",
        "\n",
        "- Make sure to include the output class (last column) as an additional input feature\n",
        "- Normalize Data\n",
        "- K = 5\n",
        "- Use 4 decimal places and DO NOT ROUND when reporting silhouette score and centroid values.\n",
        "\n",
        "\n",
        "---\n",
        "Solutions in files:\n",
        "\n",
        "[Debug HAC Single (Silhouette).txt](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/Debug%20HAC%20Single%20Link%20%28Silhouette%29.txt)\n",
        "\n",
        "[Debug HAC Complete (Silhouette).txt](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/Debug%20HAC%20Complete%20Link%20%28Silhouette%29.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_data(inputs):\n",
        "  xmin = inputs.min(axis=0)\n",
        "  xmax = inputs.max(axis=0)\n",
        "  return (inputs-xmin)/(xmax-xmin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5IBV-YkVXCRt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMPLETE LINK\n",
            "Num clusters: 5\n",
            "SSE score: 13.0824\n",
            "\n",
            "\n",
            "[0.6544,0.649 ,0.5256,0.2879,0.2815,0.3057,0.2288,0.3911]\n",
            "71\n",
            "3.8232\n",
            "\n",
            "[0.3661,0.3505,0.271 ,0.1008,0.1024,0.1058,0.0836,0.2116]\n",
            "67\n",
            "5.2786\n",
            "\n",
            "[0.7622,0.7658,0.6759,0.4265,0.4016,0.4536,0.3376,0.5217]\n",
            "38\n",
            "1.4989\n",
            "\n",
            "[0.8818,0.8904,0.7582,0.614 ,0.5433,0.5317,0.561 ,0.7794]\n",
            "16\n",
            "1.5328\n",
            "\n",
            "[0.9471,0.934 ,0.8158,0.7457,0.6434,0.7944,0.6457,0.625 ]\n",
            "8\n",
            "0.9490\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Debug Here\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff --output debug.arff\n",
        "# Train on training set\n",
        "debug_data = arff.loadarff('debug.arff')\n",
        "debug_np = np.array(debug_data[0])\n",
        "debug_norm = np.array(normalize_data(pd.DataFrame(debug_np)))\n",
        "\n",
        "# print('SINGLE LINK')\n",
        "# clf = HACClustering(k=5, link_type='single')\n",
        "# res = clf.fit(debug_norm)\n",
        "# res.score()\n",
        "\n",
        "print('COMPLETE LINK')\n",
        "clf = HACClustering(k=5, link_type='complete')\n",
        "res = clf.fit(debug_norm)\n",
        "res.score()\n",
        "res.print_clusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY3VNB1ui03N"
      },
      "source": [
        "### 1.1.3 Evaluation\n",
        "\n",
        "We will evaluate your model based on its print_clusters() output using [Evaluation Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_train.arff)\n",
        "\n",
        "- Make sure to include the output class (last column) as an additional input feature\n",
        "- Normalize Data\n",
        "- K = 5\n",
        "- Use 4 decimal places and DO NOT ROUND when reporting silhouette score and centroid values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMxalNI-XCRu"
      },
      "source": [
        "#### 1.1.3.1 Complete Link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtzHMZnHXCRu"
      },
      "source": [
        "#### 1.1.3.1 Single Link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yAxA78QjDh2"
      },
      "outputs": [],
      "source": [
        "# Load evaluation data\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_train.arff --output evaldata.arff\n",
        "eval_data = arff.loadarff('evaldata.arff')\n",
        "eval_np = np.array(eval_data[0])\n",
        "eval_norm = np.array(normalize_data(pd.DataFrame(eval_data)))\n",
        "\n",
        "# Train on evaluation data using complete link\n",
        "clf = HACClustering(k=5, link_type='complete')\n",
        "res = clf.fit(debug_norm)\n",
        "res.score()\n",
        "\n",
        "# Print clusters\n",
        "res.print_clusters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0Jh6vYeCXCRv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Alex\\OneDrive\\Desktop\\Fall2021\\CS472\\CS472-Labs\\.venv\\lib\\site-packages\\numpy\\lib\\function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "0.10710164554014481 \n",
            "\n",
            "[0.07330122 0.07919399 0.08424397 0.07975487 0.07883174 0.0627566\n",
            " 0.07713756 0.05345225]\n",
            "67\n",
            "0.04507390918155194 \n",
            "\n",
            "[0.0761997  0.07961572 0.08675442 0.07806886 0.08295861 0.13452285\n",
            " 0.07511014 0.05345225]\n",
            "2\n",
            "0.0001502428532181635 \n",
            "\n",
            "[0.07310257 0.07850764 0.08570988 0.0780761  0.08026918 0.166119\n",
            " 0.07575392 0.05345225]\n",
            "1\n",
            "0.0 \n",
            "\n",
            "[0.0937976  0.08936042 0.0846827  0.08891764 0.08948263 0.0894777\n",
            " 0.09119145 0.1069045 ]\n",
            "69\n",
            "0.06187749350537471 \n",
            "\n",
            "[0.09798201 0.09202625 0.08358246 0.09054628 0.09095389 0.16604446\n",
            " 0.09168935 0.1069045 ]\n",
            "1\n",
            "0.0 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train on evaluation data using single link\n",
        "clf = HACClustering(k=5, link_type='single')\n",
        "res = clf.fit(eval_norm)\n",
        "\n",
        "# Print clusters\n",
        "res.score()\n",
        "res.print_clusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EOkBzaUXCRv"
      },
      "source": [
        "### 1.2.1 K-Means\n",
        "\n",
        "### Code requirements \n",
        "- Ability to choose k and specify k initial centroids\n",
        "- Use Euclidean Distance as metric\n",
        "- Ability to handle distance ties\n",
        "- Include output label as a cluster feature\n",
        "\n",
        "\n",
        "---\n",
        "The output should include the following:\n",
        "- The number of clusters (k).\n",
        "- The silhouette score of the full clustering. (You can either write and use your own silhouette_score function (extra credit) or use sklearn's)\n",
        "\n",
        "\n",
        "For each cluster report include:\n",
        "\n",
        "\n",
        "- The centroid id.\n",
        "- The number of instances tied to that centroid. \n",
        "---\n",
        "You only need to handle continuous features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "J1KYATRKXCRv"
      },
      "outputs": [],
      "source": [
        "class KMEANSClustering(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self,k=3,debug=False): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            debug = if debug is true use the first k instances as the initial centroids otherwise choose random points as the initial centroids.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.debug = debug\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.centroids = X[:self.k]\n",
        "        self.clusters = [np.array([]) for k in range(self.k)]\n",
        "        is_done = False\n",
        "        while not is_done:\n",
        "            if not self.update_clusters(X):\n",
        "                is_done = True\n",
        "                break\n",
        "        return self\n",
        "\n",
        "    def update_centroids(self):\n",
        "        new_centroids = []\n",
        "        updated = False\n",
        "        for i, cluster in enumerate(self.clusters):\n",
        "            centroid = np.mean(cluster, axis=0)\n",
        "            if not np.array_equal(self.centroids[i], centroid):\n",
        "                updated = True\n",
        "            new_centroids.append(centroid)\n",
        "        self.centroids = new_centroids\n",
        "        return updated\n",
        "\n",
        "    def update_clusters(self, points):\n",
        "        self.clusters = [np.array([]) for k in range(self.k)]\n",
        "        # for each point see which centroid is closest\n",
        "        for i in range(len(points)):\n",
        "            # get shortest distance between centroids\n",
        "            min_val = np.inf\n",
        "            ind = 0\n",
        "            for j in range(len(self.centroids)):\n",
        "                dist = np.linalg.norm(points[i] - self.centroids[j])\n",
        "                if dist < min_val:\n",
        "                    min_val = dist\n",
        "                    ind = j\n",
        "            if len(self.clusters[ind]) == 0:\n",
        "                add = np.array([points[i]])\n",
        "                self.clusters[ind] = add\n",
        "            else:\n",
        "                original = self.clusters[ind]\n",
        "                original = np.append(original, np.array([points[i]]), axis=0)\n",
        "                self.clusters[ind] = original\n",
        "        \n",
        "        return self.update_centroids()\n",
        "    \n",
        "    def score(self):\n",
        "        self.sse = []\n",
        "        self.total_sse = 0\n",
        "        # find centroids\n",
        "        for i in range(len(self.clusters)):\n",
        "            # find cluster sse\n",
        "            tmp = np.linalg.norm(self.clusters[i] - self.centroids[i])\n",
        "            squared_err = tmp**2\n",
        "            self.sse.append(np.sum(squared_err))\n",
        "            self.total_sse += squared_err\n",
        "            \n",
        "    \n",
        "    def print_clusters(self):\n",
        "        \"\"\"\n",
        "            Used for grading.\n",
        "            print(\"Num clusters: {:d}\\n\".format(k))\n",
        "            print(\"Silhouette score: {:.4f}\\n\\n\".format(silhouette_score))\n",
        "            for each cluster and centroid:\n",
        "                print(np.array2string(centroid,precision=4,separator=\",\"))\n",
        "                print(\"{:d}\\n\".format(size of cluster))\n",
        "        \"\"\"\n",
        "        print(\"{:d}\".format(self.k))\n",
        "        print(\"{:.4f}\\n\".format(self.total_sse))\n",
        "        for i in range(len(self.clusters)):\n",
        "            print(np.array2string(self.centroids[i],precision=4,separator=\",\"))\n",
        "            print(\"{:d}\".format(len(self.clusters[i])))\n",
        "            print(\"{:.4f}\\n\".format(self.sse[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgGThkhwXCRv"
      },
      "source": [
        "### 1.2.2 Debug \n",
        "\n",
        "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff)\n",
        "\n",
        "\n",
        "- Train until convergence\n",
        "- Make sure to include the output class (last column) as an additional input feature\n",
        "- Normalize Data\n",
        "- K = 5\n",
        "- Use the first k instances as the initial centroids\n",
        "- Use 4 decimal places and DO NOT ROUND when reporting silhouette score and centroid values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Solutions in files:\n",
        "\n",
        "[Debug K Means (Silhouette).txt](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/Debug%20K%20Means%20%28Silhouette%29.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "KgAyy82gixIF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "9.7826\n",
            "\n",
            "[0.7325,0.7327,0.627 ,0.3817,0.3633,0.4045,0.3046,0.4839]\n",
            "75\n",
            "4.0454\n",
            "\n",
            "[0.3704,0.3519,0.2686,0.0926,0.0935,0.094 ,0.0792,0.218 ]\n",
            "34\n",
            "0.6609\n",
            "\n",
            "[0.9035,0.905 ,0.7774,0.6579,0.5767,0.6193,0.5893,0.7279]\n",
            "24\n",
            "3.2116\n",
            "\n",
            "[0.5692,0.5628,0.4376,0.211 ,0.2113,0.2248,0.1659,0.317 ]\n",
            "54\n",
            "1.5452\n",
            "\n",
            "[0.1296,0.1037,0.1053,0.0177,0.0211,0.0272,0.0135,0.0724]\n",
            "13\n",
            "0.3195\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load debug data\n",
        "clf = KMEANSClustering(k=5)\n",
        "\n",
        "# Train on debug data\n",
        "res = clf.fit(debug_norm)\n",
        "res.score()\n",
        "\n",
        "# Print clusters\n",
        "res.print_clusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh507BaLXCRw"
      },
      "source": [
        "### 1.2.3 Evaluation\n",
        "\n",
        "We will evaluate your model based on its print_clusters() output using [Evaluation Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_train.arff)\n",
        "- Train until convergence\n",
        "- Make sure to include the output class (last column) as an additional input feature\n",
        "- Normalize Data\n",
        "- K = 5\n",
        "- Use the first k instances as the initial centroids\n",
        "- Use 4 decimal places and DO NOT ROUND when reporting silhouette score and centroid values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "EGZwBrplXCRx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "0.0477\n",
            "\n",
            "[0.0926,0.0888,0.0846,0.0886,0.0889,0.083 ,0.091 ,0.1069]\n",
            "32\n",
            "0.0068\n",
            "\n",
            "[0.0738,0.0794,0.0844,0.0798,0.0792,0.0439,0.0771,0.0535]\n",
            "35\n",
            "0.0093\n",
            "\n",
            "[0.0925,0.0887,0.0846,0.0881,0.089 ,0.1257,0.0898,0.1028]\n",
            "26\n",
            "0.0186\n",
            "\n",
            "[0.0727,0.0789,0.0842,0.0796,0.0785,0.0848,0.077 ,0.0535]\n",
            "33\n",
            "0.0118\n",
            "\n",
            "[0.0968,0.0907,0.085 ,0.0901,0.0908,0.0514,0.0922,0.1069]\n",
            "14\n",
            "0.0013\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load evaluation data\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_train.arff --output k_evaldata.arff\n",
        "k_eval_data = arff.loadarff('k_evaldata.arff')\n",
        "k_eval_np = np.array(k_eval_data[0])\n",
        "k_eval_norm = preprocessing.normalize(pd.DataFrame(k_eval_np), axis=0)\n",
        "clf = KMEANSClustering(k=5)\n",
        "\n",
        "# Train on evaluation data\n",
        "res = clf.fit(k_eval_norm)\n",
        "res.score()\n",
        "\n",
        "# Print clusters\n",
        "res.print_clusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2.1.1 (7.5%) Clustering the Iris Classification problem - HAC\n",
        "\n",
        "Load the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
        "\n",
        "- Use single-link and complete link clustering algorithms\n",
        "- State whether you normalize your data or not (your choice).  \n",
        "- Show your results for clusterings using k = 2-7.  \n",
        "- Graph the silhouette score for each k and discuss your results (i.e. what kind of clusters are being made).\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Alex\\OneDrive\\Desktop\\Fall2021\\CS472\\CS472-Labs\\.venv\\lib\\site-packages\\numpy\\lib\\function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num clusters: 5\n",
            "SSE score: 0.1028\n",
            "\n",
            "\n",
            "[0.0694,0.0911,0.0289,0.014 ]\n",
            "49\n",
            "0.0076\n",
            "\n",
            "[0.0623,0.0609,0.0256,0.0173]\n",
            "1\n",
            "0.0000\n",
            "\n",
            "[0.0864,0.0756,0.0959,0.0959]\n",
            "97\n",
            "0.0951\n",
            "\n",
            "[0.0678,0.0662,0.0885,0.0978]\n",
            "1\n",
            "0.0000\n",
            "\n",
            "[0.1079,0.1006,0.1289,0.1208]\n",
            "2\n",
            "0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Iris Classification using single-link\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff --output iris.arff\n",
        "iris_data = arff.loadarff('iris.arff')\n",
        "iris_df = pd.DataFrame(iris_data[0])\n",
        "iris_df = iris_df.iloc[: , :-1]\n",
        "iris_norm = preprocessing.normalize(iris_df, axis=0)\n",
        "\n",
        "clf = HACClustering(k=5, link_type='single')\n",
        "res = clf.fit(iris_norm)\n",
        "res.score()\n",
        "\n",
        "# Print clusters\n",
        "res.print_clusters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-lcM9J9JXCRx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num clusters: 5\n",
            "SSE score: 0.0334\n",
            "\n",
            "\n",
            "[0.0671,0.0854,0.028 ,0.0115]\n",
            "33\n",
            "0.0027\n",
            "\n",
            "[0.0735,0.1003,0.0303,0.0189]\n",
            "17\n",
            "0.0022\n",
            "\n",
            "[0.0818,0.0725,0.0845,0.0762]\n",
            "52\n",
            "0.0170\n",
            "\n",
            "[0.0882,0.0762,0.104 ,0.106 ]\n",
            "23\n",
            "0.0043\n",
            "\n",
            "[0.0952,0.0832,0.1146,0.1295]\n",
            "25\n",
            "0.0072\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Iris Classification using complete-link\n",
        "clf = HACClustering(k=5, link_type='complete')\n",
        "res = clf.fit(iris_norm)\n",
        "res.score()\n",
        "\n",
        "# Print clusters\n",
        "res.print_clusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev90JsvjXCRy"
      },
      "source": [
        "Discuss differences between single-link and complete-link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myLp3nhOXCRy"
      },
      "source": [
        "## 2.2.1 (7.5%) Clustering the Iris Classification problem: K-Means\n",
        "\n",
        "Load the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
        "\n",
        "Run K-Means on the Iris dataset using the output label as a feature and without using the output label as a feature\n",
        "\n",
        "Requirements:\n",
        "- State whether you normalize your data or not (your choice).  \n",
        "- Show your results for clusterings using k = 2-7.  \n",
        "- Graph the silhouette score for each k and discuss your results (i.e. what kind of clusters are being made).\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcWCgFb4XCRz"
      },
      "outputs": [],
      "source": [
        "# Iris Classification without output label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i62njzqqXCRz"
      },
      "outputs": [],
      "source": [
        "# Iris Classification with output label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Zw0eAOXCRz"
      },
      "source": [
        "Compare results and differences between using the output label and excluding the output label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDdAadpVXCRz"
      },
      "source": [
        "## 2.2.2 (5%) Clustering the Iris Classification problem: K-Means\n",
        "\n",
        "Requirements:\n",
        "- Use the output label as an input feature\n",
        "- Run K-Means 5 times with k=4, each time with different initial random centroids and discuss any variations in the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3r8kad0XCRz"
      },
      "outputs": [],
      "source": [
        "#K-Means 5 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9_mLwXZXCRz"
      },
      "source": [
        "Discuss any variations in the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 3.1 (12.5%) Run the SK versions of HAC (both single and complete link) on iris including the output label and compare your results with those above.\n",
        "Use the silhouette score for this iris problem(k = 2-7).  You may write your own code to do silhouette (optional extra credit) or you can use sklearn.metrics.silhouette_score. Please state if you coded your own silhouette score function to receive the extra credit points (described below). Discuss how helpful Silhouette appeared to be for selecting which clustering is best. You do not need to supply full Silhouette graphs, but you could if you wanted to.\n",
        "\n",
        "Requirements\n",
        "- Use the Sillhouette score for this iris problem (k= 2-7) \n",
        "- Use at least one other scoring function from [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) and compare the results. State which metric was used. \n",
        "- Possible sklean metrics include (* metrics require ground truth labels):\n",
        "    - adjusted_mutual_info_score*\n",
        "    - adjusted_rand_score*\n",
        "    - homogeneity_score*\n",
        "    - completeness_score*\n",
        "    - fowlkes_mallows_score*\n",
        "    - calinski_harabasz_score\n",
        "    - davies_bouldin_score\n",
        "- Experiment using different hyper-parameters. Discuss Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "outputs": [],
      "source": [
        "# Load sklearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Record impressions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OWeycyLXCR0"
      },
      "source": [
        "## 3.2 (12.5%) Run the SK version of k-means on iris including the output label and compare your results with those above. \n",
        "\n",
        "Use the silhouette score for this iris problem(k = 2-7). You may write your own code to do silhouette (optional extra credit) or you can use sklearn.metrics.silhouette_score. Please state if you coded your own silhouette score function to receive the extra credit points (described below). Discuss how helpful Silhouette appeared to be for selecting which clustering is best. You do not need to supply full Silhouette graphs, but you could if you wanted to.\n",
        "\n",
        "Requirements\n",
        "- Use the Sillhouette score for this iris problem (k= 2-7) \n",
        "- Use at least one other scoring function form sklearn.metrics and compare the results. State which metric was used\n",
        "- Experiment different hyper-parameters. Discuss Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5IGOeXUXCR0"
      },
      "outputs": [],
      "source": [
        "# Load sklearn \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SFIDtKEXCR0"
      },
      "source": [
        "*Record impressions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 4. (Optional 5% extra credit) For your silhouette experiment above, write and use your own code to calculate the silhouette scores, rather than the SK or other version. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi1o5tX6XCR1"
      },
      "source": [
        "*Show findings here*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrczoZSXXCR1"
      },
      "outputs": [],
      "source": [
        "# Copy function Below"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab 1 - perceptron",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
