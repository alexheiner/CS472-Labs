{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mADd6iSDCmLs"
      },
      "source": [
        "# Decision Tree Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CGsI9XEJCmLv"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import arff as arf\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Node:\n",
        "  def __init__(self, inputs, outputs, counts, features_used=[]):\n",
        "    self.inputs = inputs\n",
        "    self.outputs = outputs\n",
        "    self.counts = counts\n",
        "    self.children = []\n",
        "    self.features_used = features_used\n",
        "    # initial info of each node\n",
        "    self.info  = self.calc_info()\n",
        "    # will be used for leaf nodes, all other nodes will have none\n",
        "    self.prediction= None\n",
        "    \n",
        "  def generate_children(self):\n",
        "    # indicating the number of columns\n",
        "    for i in range(len(self.inputs[0])):\n",
        "      # if we havent already split/seen this feature\n",
        "      if i not in self.features_used:\n",
        "        # initialize the next children for this node\n",
        "        next_children = []\n",
        "        # indicating the number of values for this feature\n",
        "        for j in range(self.counts[i]):\n",
        "          # initialize temp inputs and outputs. we will use these arrays to set as inputs and outputs for the next node\n",
        "          tmp_inputs = []\n",
        "          tmp_outputs = []\n",
        "          # loop though all of the inputs to add to the tmp arrays\n",
        "          for k in range(len(self.inputs)):\n",
        "            # filtering the input values we are adding to tmp arrays\n",
        "            if self.inputs[k][i] == j:\n",
        "              tmp_inputs.append(self.inputs[k])\n",
        "              tmp_outputs.append(self.outputs[k])\n",
        "          # if there were inputs to add, create a new node\n",
        "          if len(tmp_inputs) != 0:\n",
        "            new_node = Node(tmp_inputs, tmp_outputs, self.counts, np.concatenate((self.features_used, [i])))\n",
        "            # setting the feature and feature index, keeping track of where we are so we know where to go\n",
        "            new_node.set_feature(j)\n",
        "            new_node.set_feature_index(i)\n",
        "            next_children.append(new_node)\n",
        "        if len(self.children) == 0:\n",
        "          self.children = next_children\n",
        "        else:\n",
        "          # get the current split info and next split info\n",
        "          curr_split_gain = self.calc_children_info(self.children)\n",
        "          next_split_gain = self.calc_children_info(next_children)\n",
        "          # if there is more info gain on next split, split here with this nodes children\n",
        "          if next_split_gain > curr_split_gain:\n",
        "            self.children = next_children\n",
        "    return self.calc_children_info(self.children)\n",
        "\n",
        "  def calc_children_info(self, children):\n",
        "    info = self.info\n",
        "    for i in range(len(children)):\n",
        "      info -= (len(children[i].inputs) / len(self.inputs) * children[i].get_info())\n",
        "    return info\n",
        "    \n",
        "  def set_feature(self, num):\n",
        "    self.feature = num\n",
        "    return\n",
        "\n",
        "  def set_feature_index(self, num):\n",
        "    self.feature_index = num\n",
        "    return\n",
        "\n",
        "  def get_num_feat_used(self):\n",
        "    return len(self.features_used)\n",
        "\n",
        "  def predict_node(self):\n",
        "    # if info is zero, prediction will be first index of nodes output array\n",
        "    if self.get_info() == 0:\n",
        "      self.prediction = self.outputs[0]\n",
        "    else:\n",
        "      self.prediction = stats.mode(self.inputs).mode[0][0]\n",
        "\n",
        "  def get_children(self):\n",
        "    return self.children\n",
        "\n",
        "  def calc_info(self):\n",
        "    info = 0\n",
        "    # calculate information gained\n",
        "    ent_arr = np.zeros(self.counts[-1])\n",
        "    for i in range(len(self.outputs)):\n",
        "      ent_arr[self.outputs[i]] += 1\n",
        "    ent_arr = ent_arr / len(self.inputs)\n",
        "    for child in ent_arr:\n",
        "      if child != 0:\n",
        "          info += (-child) * math.log(child, 2)\n",
        "    if len(ent_arr) == 0:\n",
        "      self.prediction = self.outputs[0]\n",
        "    return info\n",
        "\n",
        "  def get_info(self):\n",
        "    return self.info\n",
        "\n",
        "  def get_attr_index(self):\n",
        "    return self.children_attr_index[self.index]\n",
        "\n",
        "  # passing -1 for layer of root node\n",
        "  def print_tree(self, layer):\n",
        "    # need to print a indentation for the number of layer we are on\n",
        "    for i in range(layer):\n",
        "      print(\"   \",end=\"\")\n",
        "    # if we are passed the root node, print out the feature and feature index\n",
        "    if layer >= 0:\n",
        "      print('feature ', end=\"\")\n",
        "      print(self.feature_index, end=\"\")\n",
        "      print(' = ', end=\"\")\n",
        "      print(self.feature, end=\"\")\n",
        "      print(':')\n",
        "    # if we are at a leaf node, print out the prediction\n",
        "    if self.prediction != None:\n",
        "      for i in range(layer+1):\n",
        "        print(\"   \", end=\"\")\n",
        "      print('prediction: ' + str(self.prediction))\n",
        "    else:\n",
        "      # loop through all children and print out information\n",
        "      num_ch = len(self.get_children())\n",
        "      for i in range(num_ch):\n",
        "        self.get_children()[i].print_tree(layer+1)\n",
        "    return\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgq5w2mOCmLw"
      },
      "source": [
        "## 1. (40%) Correctly implement the ID3 decision tree algorithm, including the ability to handle unknown attributes (You do not need to handle real valued attributes).  \n",
        "### Code Requirements/Notes:\n",
        "- Use standard information gain as your basic attribute evaluation metric.  (Note that normal ID3 would usually augment information gain with gain ratio or some other mechanism to penalize statistically insignificant attribute splits. Otherwise, even with approaches like pruning below, the SSE type of overfit could still hurt us.) \n",
        "- You are welcome to create other classes and/or functions in addition to the ones provided below. (e.g. If you build out a tree structure, you might create a node class).\n",
        "- It is a good idea to use a simple data set (like the lenses data or the pizza homework), which you can check by hand, to test your algorithm to make sure that it is working correctly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XtB9lCVaCmLw"
      },
      "outputs": [],
      "source": [
        "class DTClassifier(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self,counts=None):\n",
        "        \"\"\" Initialize class with chosen hyperparameters.\n",
        "        Args:\n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            counts: A list of Ints that tell you how many types of each feature there are\n",
        "        Example:\n",
        "            DT  = DTClassifier()\n",
        "            or\n",
        "            DT = DTClassifier(count = [2,3,2,2])\n",
        "            Dataset = \n",
        "            [[0,1,0,0],\n",
        "            [1,2,1,1],\n",
        "            [0,1,1,0],\n",
        "            [1,2,0,1],\n",
        "            [0,0,1,1]]\n",
        "\n",
        "        \"\"\" \n",
        "        self.counts = counts\n",
        "        # used to save info gain through the fit process\n",
        "        self.info_gain =[]\n",
        "\n",
        "    def print_tree(self):\n",
        "        self.root_node.print_tree(-1)\n",
        "\n",
        "    def split_and_fit(self, fullData):\n",
        "        # split up targets and inputs\n",
        "        train = np.array(fullData[:,0:-1])\n",
        "        targets = np.array(fullData[:,-1])\n",
        "        # fit data\n",
        "        self.fit(train, targets)\n",
        "        return\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit the data; Make the Decision tree\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 1D numpy array with the training targets\n",
        "\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "\n",
        "        \"\"\"\n",
        "        # set number of features and root node class variables\n",
        "        self.num_features = len(X[0])\n",
        "        self.root_node = Node(X, y, self.counts, features_used=[])\n",
        "\n",
        "        # start splitting the tree from root node\n",
        "        self.split_tree(self.root_node)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def split_tree(self, node):\n",
        "        # how we know when we are done\n",
        "        if node.get_info() != 0 and node.get_num_feat_used() < self.num_features:\n",
        "            # get info gained at the split\n",
        "            info = node.generate_children()\n",
        "            self.info_gain.append(info)\n",
        "            # get children of current node\n",
        "            children = node.get_children()\n",
        "            for i in range(len(children)):\n",
        "                # loop through children to split tree\n",
        "                if len(node.get_children()[i].inputs) != 0:\n",
        "                    self.split_tree(node.get_children()[i])\n",
        "        else:\n",
        "            # at a leaf node, need to do prediction\n",
        "            node.predict_node()\n",
        "    \n",
        "    def predict_helper(self, node, pattern):\n",
        "        # BASE CASE- if we are at the leaf node return its prediction\n",
        "        if node.prediction != None:\n",
        "            return node.prediction\n",
        "        else:\n",
        "            children = node.get_children()\n",
        "            feat_ind = children[0].feature_index\n",
        "            pat_val = pattern[feat_ind]\n",
        "            n_node = None\n",
        "            for i in range(len(children)):\n",
        "                if children[i].feature == pat_val:\n",
        "                    n_node = children[i]\n",
        "                    break\n",
        "            # need to choose a random child node incase we aren't done yet\n",
        "            if n_node == None:\n",
        "                rand = random.randint(0, len(children) - 1)\n",
        "                n_node = children[rand]\n",
        "        # recursive call to traverse tree\n",
        "        return self.predict_helper(n_node, pattern)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict all classes for a dataset X\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        preds = []\n",
        "        # append predictions of each row in x by traversing tree\n",
        "        for i in range(len(X)):\n",
        "            preds.append(self.predict_helper(self.root_node, X[i]))\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\" Return accuracy(Classification Acc) of model on a given dataset. Must implement own score function.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 1D numpy array of the targets \n",
        "        \"\"\"\n",
        "        # get predictions array\n",
        "        predictions = self.predict(X)\n",
        "        total = len(y)\n",
        "        correct = 0\n",
        "        # compare with actual outputs\n",
        "        for i in range(len(predictions)):\n",
        "            if predictions[i] == y[i]:\n",
        "                correct +=1\n",
        "        # return percentage correct\n",
        "        return correct/total\n",
        "        \n",
        "    # returns array of info gain at each split \n",
        "    def get_split_info_gain(self):\n",
        "        return self.info_gain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def convertBytestoString(df):\n",
        "  for col in df:\n",
        "    if isinstance(df[col][0], bytes):\n",
        "      df[col] = df[col].str.decode(\"utf8\")\n",
        "  return df\n",
        "\n",
        "def get_counts(dataset):\n",
        "  counts = []\n",
        "  for column in dataset:\n",
        "    counts.append(len(dataset[column].value_counts()))\n",
        "  return counts\n",
        "\n",
        "def handle_missing(dataset):\n",
        "  # calculate mode, replacing missing values with mode\n",
        "  mode = dataset.mode().to_numpy().flatten()\n",
        "  data_columns = len(dataset.columns.to_numpy())\n",
        "\n",
        "  new_dataset = dataset.to_numpy()\n",
        "  length = len(new_dataset)\n",
        "  for i in range(length):\n",
        "    # loop through columns looking for missing values\n",
        "    for j in range(data_columns):\n",
        "      # missing with question mark and np.nan\n",
        "      if new_dataset[i][j] == '?' or new_dataset[i][j] == np.nan:\n",
        "        new_dataset[i][j] = mode[j]\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for cleaning lenses dataset\n",
        "lenses_dict = {\"age\": {\"young\": 2, \"pre_presbyopic\": 1, \"presbyopic\": 0},\n",
        "    \"spectacle_prescrip\": {\"myope\": 1, \"hypermetrope\": 0},\n",
        "    \"astigmatism\": {\"no\": 0, \"yes\": 1},\n",
        "    \"tear_prod_rate\" : {\"reduced\" : 1, \"normal\" : 0 },\n",
        "    \"contact_lenses\" : {\"none\": 1, \"soft\": 2, \"hard\": 0}}\n",
        "\n",
        "# for cleaning zoo dataset\n",
        "zoo_dict = {\"hair\":     {\"F\": 0, \"T\": 1},\n",
        "    \"feathers\": {\"F\": 0, \"T\": 1},\n",
        "    \"eggs\": {\"F\": 0, \"T\": 1},\n",
        "    \"milk\": {\"F\": 0, \"T\": 1},\n",
        "    \"airborne\": {\"F\": 0, \"T\": 1},\n",
        "    \"predator\": {\"F\": 0, \"T\": 1},\n",
        "    \"aquatic\": {\"F\": 0, \"T\": 1},\n",
        "    \"toothed\": {\"F\": 0, \"T\": 1},\n",
        "    \"backbone\": {\"F\": 0, \"T\": 1},\n",
        "    \"breathes\": {\"F\": 0, \"T\": 1},\n",
        "    \"venomous\": {\"F\": 0, \"T\": 1},\n",
        "    \"fins\": {\"F\": 0, \"T\": 1},\n",
        "    \"legs\": {\"0\": 0, \"2\": 1, \"4\": 2, \"5\": 3, \"6\": 4, \"8\": 5},\n",
        "    \"tails\": {\"F\": 0, \"T\": 1},\n",
        "    \"domestic\": {\"F\": 0, \"T\": 1},\n",
        "    \"catsize\": {\"F\": 0, \"T\": 1},\n",
        "    \"type\": {\"cT\": 0, \"c2\": 1, \"c3\": 2, \"c4\": 3, \"c5\": 4, \"c6\": 5,\"c7\": 6}}\n",
        "\n",
        "# for cleaning cars dataset\n",
        "cars_dict = {\"buying\": {\"vhigh\": 0, \"high\": 1, \"med\": 2, \"low\": 3},\n",
        "    \"maint\": {\"vhigh\": 0, \"high\": 1, \"med\": 2, \"low\": 3},\n",
        "    \"doors\": {\"2\": 0, \"3\": 1, \"4\": 2, \"5more\": 3},\n",
        "    \"persons\": {\"2\": 0, \"4\": 1, \"more\": 2},\n",
        "    \"lug_boot\": {\"small\": 0, \"med\": 1, \"big\": 2},\n",
        "    \"safety\": {\"low\": 0, \"med\": 1, \"high\": 2},\n",
        "    \"class\": {\"unacc\": 0, \"acc\": 1, \"good\": 2, \"vgood\": 3}}\n",
        "\n",
        "car_columns = [\"buying\",\"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
        "\n",
        "# for cleaning voting dataset\n",
        "voting_dict = {'handicapped-infants': { 'n':0, 'y':1},\n",
        "                'water-project-cost-sharing' : { 'n':0, 'y':1},\n",
        "                'adoption-of-the-budget-resolution' : { 'n':0, 'y':1},\n",
        "                'physician-fee-freeze' : { 'n':0, 'y':1},\n",
        "                'el-salvador-aid' : { 'n':0, 'y':1},\n",
        "                'religious-groups-in-schools': { 'n':0, 'y':1},\n",
        "                'anti-satellite-test-ban' : { 'n':0, 'y':1},\n",
        "                'aid-to-nicaraguan-contras' : { 'n':0, 'y':1},\n",
        "                'mx-missile' : { 'n':0, 'y':1},\n",
        "                'immigration' : { 'n':0, 'y':1},\n",
        "                'synfuels-corporation-cutback' : { 'n':0, 'y':1},\n",
        "                'education-spending' : { 'n':0, 'y':1},\n",
        "                'superfund-right-to-sue' : { 'n':0, 'y':1},\n",
        "                'crime' : { 'n':0, 'y':1},\n",
        "                'duty-free-exports' : { 'n':0, 'y':1},\n",
        "                'export-administration-act-south-africa' : { 'n':0, 'y':1},\n",
        "                'Class' : { 'democrat':0, 'republican':1}}\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybGas3ACmLx"
      },
      "source": [
        "## 1.1 Debug\n",
        "\n",
        "Debug your model by training on the lenses dataset: [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses.arff)\n",
        "\n",
        "Test your model on the lenses test set: [Debug Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses_test.arff)\n",
        "\n",
        "Parameters:\n",
        "(optional) counts = [3,2,2,2] (You should compute this when you read in the data, before fitting)\n",
        "\n",
        "---\n",
        "\n",
        "Expected Results: Accuracy = [0.33]\n",
        "\n",
        "Predictions should match this file: [Lenses Predictions](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv)\n",
        "\n",
        "*NOTE: The [Lenses Prediction](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv) uses the following encoding: soft=2, hard=0, none=1. If your encoding is different, then your output will be different, but not necessarily incorrect.*\n",
        "\n",
        "Split Information Gains (These do not need to be in this exact order):\n",
        "\n",
        "[0.5487949406953987, 0.7704260414863775, 0.3166890883150208, 1.0, 0.4591479170272447, 0.9182958340544894]\n",
        "\n",
        "<!-- You should be able to get about 68% (61%-82%) predictive accuracy on the lenses data -->\n",
        "\n",
        "Here's what your decision tree splits should look like, and the corresponding child node predictions:\n",
        "\n",
        "Decision Tree:\n",
        "<pre>\n",
        "tear_prod_rate = normal:\n",
        "    astigmatism = no:\n",
        "        age = pre_presbyopic:\n",
        "            prediction: soft\n",
        "        age = presbyopic:\n",
        "            spectacle_prescrip = hypermetrope:\n",
        "                prediction: soft\n",
        "            spectacle_prescrip = myope:\n",
        "                prediction: none\n",
        "        age = young:\n",
        "            prediction: soft\n",
        "    astigmatism = yes:\n",
        "        spectacle_prescrip = hypermetrope:\n",
        "            age = pre_presbyopic:\n",
        "                prediction: none\n",
        "            age = presbyopic:\n",
        "                prediction: none\n",
        "            age = young:\n",
        "                prediction: hard\n",
        "        spectacle_prescrip = myope:\n",
        "            prediction: hard\n",
        "tear_prod_rate = reduced:\n",
        "    prediction: none\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSNiejBECmLy",
        "outputId": "fbc5a652-3c8e-479d-a0a6-28b1d184c83e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature 3 = 0:\n",
            "   feature 2 = 0:\n",
            "      feature 0 = 0:\n",
            "         feature 1 = 0:\n",
            "            prediction: 2\n",
            "         feature 1 = 1:\n",
            "            prediction: 1\n",
            "      feature 0 = 1:\n",
            "         prediction: 2\n",
            "      feature 0 = 2:\n",
            "         prediction: 2\n",
            "   feature 2 = 1:\n",
            "      feature 1 = 0:\n",
            "         feature 0 = 0:\n",
            "            prediction: 1\n",
            "         feature 0 = 1:\n",
            "            prediction: 1\n",
            "         feature 0 = 2:\n",
            "            prediction: 0\n",
            "      feature 1 = 1:\n",
            "         prediction: 0\n",
            "feature 3 = 1:\n",
            "   prediction: 1\n",
            "\n",
            "\n",
            "acc:  0.3333333333333333 \n",
            "\n",
            "Split info gain\n",
            "[0.5487949406953985, 0.7704260414863777, 0.3166890883150208, 1.0, 0.4591479170272448, 0.9182958340544896]\n"
          ]
        }
      ],
      "source": [
        "# Load debug training data \n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses.arff --output lenses.arff\n",
        "    \n",
        "debug_data = arff.loadarff('lenses.arff')\n",
        "lenses_df = pd.DataFrame(debug_data[0])\n",
        "lenses_df = convertBytestoString(lenses_df)\n",
        "lenses_df = lenses_df.replace(lenses_dict)\n",
        "\n",
        "lenses_np = lenses_df.to_numpy()\n",
        "counts = get_counts(lenses_df)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DTClassifier(counts=counts)\n",
        "acc = clf.split_and_fit(lenses_np)\n",
        "clf.print_tree()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Load debug test data\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses_test.arff --output debug_test.arff\n",
        "\n",
        "debug_test = arff.loadarff('debug_test.arff')\n",
        "debug_test_df = pd.DataFrame(debug_test[0])\n",
        "\n",
        "lenses_test_df = convertBytestoString(debug_test_df)\n",
        "lenses_test_df = lenses_test_df.replace(lenses_dict)\n",
        "\n",
        "# Convert to numpy\n",
        "lenses_np_test = lenses_test_df.to_numpy()\n",
        "\n",
        "# Predict and compute model accuracy\n",
        "prediction = clf.predict(lenses_np_test[:,0:len(counts)-1])\n",
        "acc = clf.score(lenses_np_test[:,0:], lenses_np_test[:,len(counts)-1:])\n",
        "print(\"acc: \", acc, \"\\n\")\n",
        "\n",
        "# Print the information gain of every split you make.\n",
        "print(\"Split info gain\")\n",
        "print(clf.get_split_info_gain())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGU8PK2xCmLz"
      },
      "source": [
        "## 1.2 Evaluation\n",
        "\n",
        "We will evaluate your model based on its performance on the zoo dataset. \n",
        "\n",
        "Train your model using this dataset: [Evaluation Train Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo.arff)\n",
        "\n",
        "Test your model on this dataset: [Evaluation Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo_test.arff)\n",
        "\n",
        "Parameters:\n",
        "(optional) counts = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2] (You should compute this when you read in the data, before fitting)\n",
        "\n",
        "---\n",
        "Print out your accuracy on the evaluation test dataset.\n",
        "\n",
        "Print out the information gain of every split you make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xBr3I6jXCmL0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature 12 = 0:\n",
            "   feature 11 = 0:\n",
            "      feature 7 = 0:\n",
            "         prediction: 6\n",
            "      feature 7 = 1:\n",
            "         prediction: 2\n",
            "   feature 11 = 1:\n",
            "      feature 2 = 0:\n",
            "         prediction: 0\n",
            "      feature 2 = 1:\n",
            "         prediction: 3\n",
            "feature 12 = 1:\n",
            "   feature 0 = 0:\n",
            "      prediction: 1\n",
            "   feature 0 = 1:\n",
            "      prediction: 0\n",
            "feature 12 = 2:\n",
            "   feature 0 = 0:\n",
            "      feature 5 = 0:\n",
            "         prediction: 2\n",
            "      feature 5 = 1:\n",
            "         feature 7 = 0:\n",
            "            prediction: 6\n",
            "         feature 7 = 1:\n",
            "            prediction: 4\n",
            "   feature 0 = 1:\n",
            "      prediction: 0\n",
            "feature 12 = 3:\n",
            "   prediction: 6\n",
            "feature 12 = 4:\n",
            "   feature 5 = 0:\n",
            "      prediction: 5\n",
            "   feature 5 = 1:\n",
            "      prediction: 6\n",
            "feature 12 = 5:\n",
            "   prediction: 6\n",
            "\n",
            "\n",
            "acc:  0.147 \n",
            "\n",
            "Split info gain\n",
            "[1.3630469031539396, 0.8865408928220901, 0.9852281360342516, 0.6962122601251458, 0.8256265261578954, 0.6892019851173655, 0.863120568566631, 0.7219280948873623, 0.7219280948873623]\n"
          ]
        }
      ],
      "source": [
        "# Load evaluation training data\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo.arff --output zoo.arff\n",
        "    \n",
        "eval_data = arff.loadarff('zoo.arff')\n",
        "eval_df = pd.DataFrame(eval_data[0])\n",
        "eval_df = convertBytestoString(eval_df)\n",
        "eval_df = eval_df.replace(zoo_dict)\n",
        "eval_np = eval_df.to_numpy()\n",
        "counts = get_counts(eval_df)\n",
        "\n",
        "# Train Decision Tree\n",
        "eval_clf = DTClassifier(counts=counts)\n",
        "eval_acc = eval_clf.split_and_fit(eval_np)\n",
        "eval_clf.print_tree()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Load debug test data\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo_test.arff --output eval_test.arff\n",
        "\n",
        "eval_test = arff.loadarff('eval_test.arff')\n",
        "eval_test_df = pd.DataFrame(eval_test[0])\n",
        "\n",
        "eval_test_df = convertBytestoString(eval_test_df)\n",
        "eval_test_df = eval_test_df.replace(zoo_dict)\n",
        "\n",
        "eval_np_test = eval_test_df.to_numpy()\n",
        "\n",
        "# Predict and compute model accuracy\n",
        "prediction = eval_clf.predict(eval_np_test[:,0:len(counts)-1])\n",
        "acc = eval_clf.score(eval_np_test[:,0:], eval_np_test[:,len(counts)-1:])\n",
        "print(\"acc: \", acc, \"\\n\")\n",
        "\n",
        "# Print the information gain of every split you make.\n",
        "print(\"Split info gain\")\n",
        "print(eval_clf.get_split_info_gain())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVfaMvqHCmL0"
      },
      "source": [
        "## 2. (20%) You will use your ID3 algorithm to induce decision trees for the cars dataset and the voting dataset.  Do not use a stopping criteria, but induce the tree as far as it can go (until classes are pure or there are no more data or attributes to split on).  \n",
        "- Implement and use 10-fold Cross Validation (CV) on each data set to predict how well the models will do on novel data.  \n",
        "- For each dataset, report the training and test classification accuracy for each fold and the average test accuracy. \n",
        "- As a rough sanity check, typical decision tree accuracies for these data sets are: Cars: .90-.95, Vote: .92-.95."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQdySr-HCmL0"
      },
      "source": [
        "## 2.1 Implement 10-fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "RDRcKCYjCmL1"
      },
      "outputs": [],
      "source": [
        "# Write a function that implements 10-fold cross validation\n",
        "class K_Fold_CV():\n",
        "  def __init__(self, dataset, k_val):\n",
        "    self.dataset = dataset\n",
        "    self.k_val = k_val\n",
        "    # split and shuffle is automatically called from the constructor\n",
        "    self.split_data = self.shuffle_and_split()\n",
        "\n",
        "  def shuffle_and_split(self):\n",
        "    # shuffle before folding\n",
        "    np.random.shuffle(self.dataset)\n",
        "    # initialize array for splitting\n",
        "    split_data = [[] for x in range(self.k_val)]\n",
        "    split_pointer = 0\n",
        "    # loop through dataset passed in\n",
        "    for i in range(len(self.dataset)):\n",
        "      split_data[split_pointer].append(self.dataset[i])\n",
        "      split_pointer += 1\n",
        "      # reset pointer\n",
        "      if split_pointer == self.k_val:\n",
        "        split_pointer = 0\n",
        "    return split_data\n",
        "\n",
        "  def get_datasets_at_index(self, k_index):\n",
        "    # initialize test dataset\n",
        "    t_set = []\n",
        "    for i in range(self.k_val):\n",
        "      if i != k_index:\n",
        "        for j in range(len(self.split_data[i])):\n",
        "          # append to test set\n",
        "          t_set.append(self.split_data[i][j])\n",
        "    # get validation set from split data based on k_index\n",
        "    v_set = self.split_data[k_index]\n",
        "    # convert to np array before returning\n",
        "    return np.array(t_set), np.array(v_set)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrurQtTTCmL1"
      },
      "source": [
        "##  2.2 Cars Dataset\n",
        "- Use this [Cars Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/cars.arff)\n",
        "- Make a table for your K-Fold cross validation accuracies\n",
        "\n",
        "*If you are having trouble using scipy's loadarff function (scipy.io.arff.loadarff), try:*\n",
        "\n",
        "*pip install arff &nbsp;&nbsp;&nbsp;&nbsp;          # Install arff library*\n",
        "\n",
        "*import arff as arf*                   \n",
        "\n",
        "*cars = list(arf.load('cars.arff'))   &nbsp;&nbsp;&nbsp;&nbsp;# Load your downloaded dataset (!curl, etc.)*\n",
        "\n",
        "*df = pd.DataFrame(cars)*  \n",
        "\n",
        "*There may be additional cleaning needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "6I50wXJeCmL1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature 5 = 0:\n",
            "   prediction: 0\n",
            "feature 5 = 1:\n",
            "   feature 3 = 0:\n",
            "      prediction: 0\n",
            "   feature 3 = 1:\n",
            "      feature 0 = 0:\n",
            "         feature 1 = 0:\n",
            "            prediction: 0\n",
            "         feature 1 = 1:\n",
            "            prediction: 0\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 0\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "      feature 0 = 1:\n",
            "         feature 4 = 0:\n",
            "            prediction: 0\n",
            "         feature 4 = 1:\n",
            "            feature 2 = 0:\n",
            "               prediction: 0\n",
            "            feature 2 = 1:\n",
            "               prediction: 0\n",
            "            feature 2 = 2:\n",
            "               feature 1 = 0:\n",
            "                  prediction: 0\n",
            "               feature 1 = 1:\n",
            "                  prediction: 1\n",
            "               feature 1 = 2:\n",
            "                  prediction: 1\n",
            "               feature 1 = 3:\n",
            "                  prediction: 1\n",
            "            feature 2 = 3:\n",
            "               feature 1 = 0:\n",
            "                  prediction: 0\n",
            "               feature 1 = 1:\n",
            "                  prediction: 1\n",
            "               feature 1 = 3:\n",
            "                  prediction: 1\n",
            "         feature 4 = 2:\n",
            "            feature 1 = 0:\n",
            "               prediction: 0\n",
            "            feature 1 = 1:\n",
            "               prediction: 1\n",
            "            feature 1 = 2:\n",
            "               prediction: 1\n",
            "            feature 1 = 3:\n",
            "               prediction: 1\n",
            "      feature 0 = 2:\n",
            "         feature 1 = 0:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 0\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 1:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 0\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 2:\n",
            "            prediction: 1\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 2:\n",
            "               prediction: 2\n",
            "      feature 0 = 3:\n",
            "         feature 1 = 0:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 0\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 1:\n",
            "            prediction: 1\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 2:\n",
            "               prediction: 2\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 2:\n",
            "               prediction: 2\n",
            "   feature 3 = 2:\n",
            "      feature 0 = 0:\n",
            "         feature 1 = 0:\n",
            "            prediction: 0\n",
            "         feature 1 = 1:\n",
            "            prediction: 0\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "      feature 0 = 1:\n",
            "         feature 4 = 0:\n",
            "            prediction: 0\n",
            "         feature 4 = 1:\n",
            "            feature 1 = 0:\n",
            "               prediction: 0\n",
            "            feature 1 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 1 = 2:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 1 = 3:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "         feature 4 = 2:\n",
            "            feature 1 = 0:\n",
            "               prediction: 0\n",
            "            feature 1 = 1:\n",
            "               prediction: 1\n",
            "            feature 1 = 2:\n",
            "               prediction: 1\n",
            "            feature 1 = 3:\n",
            "               prediction: 1\n",
            "      feature 0 = 2:\n",
            "         feature 1 = 0:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 1:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 2:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 2:\n",
            "               prediction: 2\n",
            "      feature 0 = 3:\n",
            "         feature 1 = 0:\n",
            "            feature 4 = 0:\n",
            "               prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 2:\n",
            "               prediction: 1\n",
            "         feature 1 = 1:\n",
            "            prediction: 1\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 2:\n",
            "               prediction: 2\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "               feature 2 = 3:\n",
            "                  prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 2:\n",
            "               prediction: 2\n",
            "feature 5 = 2:\n",
            "   feature 3 = 0:\n",
            "      prediction: 0\n",
            "   feature 3 = 1:\n",
            "      feature 0 = 0:\n",
            "         feature 1 = 0:\n",
            "            prediction: 0\n",
            "         feature 1 = 1:\n",
            "            prediction: 0\n",
            "         feature 1 = 2:\n",
            "            prediction: 1\n",
            "         feature 1 = 3:\n",
            "            prediction: 1\n",
            "      feature 0 = 1:\n",
            "         feature 1 = 0:\n",
            "            prediction: 0\n",
            "         feature 1 = 1:\n",
            "            prediction: 1\n",
            "         feature 1 = 2:\n",
            "            prediction: 1\n",
            "         feature 1 = 3:\n",
            "            prediction: 1\n",
            "      feature 0 = 2:\n",
            "         feature 1 = 0:\n",
            "            prediction: 1\n",
            "         feature 1 = 1:\n",
            "            prediction: 1\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               prediction: 2\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 2\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "      feature 0 = 3:\n",
            "         feature 1 = 0:\n",
            "            prediction: 1\n",
            "         feature 1 = 1:\n",
            "            feature 4 = 0:\n",
            "               prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               prediction: 2\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 2\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               prediction: 2\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "   feature 3 = 2:\n",
            "      feature 0 = 0:\n",
            "         feature 1 = 0:\n",
            "            prediction: 0\n",
            "         feature 1 = 1:\n",
            "            prediction: 0\n",
            "         feature 1 = 2:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "         feature 1 = 3:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "      feature 0 = 1:\n",
            "         feature 1 = 0:\n",
            "            prediction: 0\n",
            "         feature 1 = 1:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "         feature 1 = 2:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "         feature 1 = 3:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "      feature 0 = 2:\n",
            "         feature 1 = 0:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "         feature 1 = 1:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 2\n",
            "               feature 2 = 1:\n",
            "                  prediction: 3\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "      feature 0 = 3:\n",
            "         feature 1 = 0:\n",
            "            feature 2 = 0:\n",
            "               feature 4 = 0:\n",
            "                  prediction: 0\n",
            "               feature 4 = 1:\n",
            "                  prediction: 1\n",
            "               feature 4 = 2:\n",
            "                  prediction: 1\n",
            "            feature 2 = 1:\n",
            "               prediction: 1\n",
            "            feature 2 = 2:\n",
            "               prediction: 1\n",
            "            feature 2 = 3:\n",
            "               prediction: 1\n",
            "         feature 1 = 1:\n",
            "            feature 4 = 0:\n",
            "               prediction: 1\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 1\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "         feature 1 = 2:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 2\n",
            "               feature 2 = 1:\n",
            "                  prediction: 3\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "         feature 1 = 3:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 0\n",
            "               feature 2 = 1:\n",
            "                  prediction: 2\n",
            "               feature 2 = 2:\n",
            "                  prediction: 2\n",
            "               feature 2 = 3:\n",
            "                  prediction: 2\n",
            "            feature 4 = 1:\n",
            "               feature 2 = 0:\n",
            "                  prediction: 2\n",
            "               feature 2 = 1:\n",
            "                  prediction: 3\n",
            "               feature 2 = 2:\n",
            "                  prediction: 3\n",
            "               feature 2 = 3:\n",
            "                  prediction: 3\n",
            "            feature 4 = 2:\n",
            "               prediction: 3\n",
            "test acc:  [0.9595375722543352, 0.9190751445086706, 0.9479768786127167, 0.9190751445086706, 0.930635838150289, 0.9479768786127167, 0.9190751445086706, 0.9190751445086706, 0.9534883720930233, 0.9186046511627907]\n",
            "avg acc:  0.9334520768920553\n"
          ]
        }
      ],
      "source": [
        "# Use 10-fold CV on Cars Dataset\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/cars.arff --output cars.arff\n",
        "\n",
        "cars = list(arf.load('cars.arff'))\n",
        "cars_df = pd.DataFrame(cars, columns=car_columns)\n",
        "cars_df = convertBytestoString(cars_df)\n",
        "cars_df = cars_df.replace(cars_dict)\n",
        "\n",
        "cars_np = cars_df.to_numpy()\n",
        "counts = get_counts(cars_df)\n",
        "\n",
        "k_splits = 10\n",
        "cv_tracter = K_Fold_CV(cars_np, k_splits)\n",
        "cv_accs = []\n",
        "\n",
        "for i in range(k_splits):\n",
        "    t_set, v_set = cv_tracter.get_datasets_at_index(i)\n",
        "    tmp_clf = DTClassifier(counts=counts)\n",
        "    tmp_clf.split_and_fit(t_set)\n",
        "    if i == 0:\n",
        "        tmp_clf.print_tree()\n",
        "    acc = tmp_clf.score(v_set[:,0:], v_set[:,len(counts)-1:])\n",
        "    cv_accs.append(acc)\n",
        "\n",
        "# Report Training and Test Classification Accuracies\n",
        "print(\"test acc: \", cv_accs)\n",
        "\n",
        "# Report Average Test Accuracy\n",
        "avg_acc = sum(cv_accs)/k_splits\n",
        "print(\"avg acc: \", avg_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z93wyVhjCmL1"
      },
      "source": [
        "## 2.3 Voting Dataset\n",
        "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)\n",
        "- Note that you will need to support unknown attributes in the voting data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "yUNtg8LBCmL2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature 3 = 0:\n",
            "   feature 10 = 0:\n",
            "      feature 13 = 0:\n",
            "         feature 6 = 0:\n",
            "            feature 0 = 0:\n",
            "               prediction: 1\n",
            "            feature 0 = 1:\n",
            "               prediction: 0\n",
            "         feature 6 = 1:\n",
            "            prediction: 0\n",
            "      feature 13 = 1:\n",
            "         feature 6 = 0:\n",
            "            prediction: 0\n",
            "         feature 6 = 1:\n",
            "            feature 4 = 0:\n",
            "               feature 2 = 0:\n",
            "                  feature 0 = 0:\n",
            "                     prediction: 0\n",
            "                  feature 0 = 1:\n",
            "                     prediction: 1\n",
            "               feature 2 = 1:\n",
            "                  prediction: 0\n",
            "            feature 4 = 1:\n",
            "               feature 0 = 0:\n",
            "                  feature 5 = 0:\n",
            "                     prediction: 0\n",
            "                  feature 5 = 1:\n",
            "                     prediction: 1\n",
            "               feature 0 = 1:\n",
            "                  prediction: 0\n",
            "   feature 10 = 1:\n",
            "      prediction: 0\n",
            "feature 3 = 1:\n",
            "   feature 10 = 0:\n",
            "      feature 14 = 0:\n",
            "         feature 2 = 0:\n",
            "            prediction: 1\n",
            "         feature 2 = 1:\n",
            "            feature 9 = 0:\n",
            "               feature 6 = 0:\n",
            "                  prediction: 0\n",
            "               feature 6 = 1:\n",
            "                  prediction: 1\n",
            "            feature 9 = 1:\n",
            "               prediction: 1\n",
            "      feature 14 = 1:\n",
            "         feature 9 = 0:\n",
            "            feature 6 = 0:\n",
            "               feature 11 = 0:\n",
            "                  prediction: 0\n",
            "               feature 11 = 1:\n",
            "                  prediction: 1\n",
            "            feature 6 = 1:\n",
            "               prediction: 1\n",
            "         feature 9 = 1:\n",
            "            prediction: 1\n",
            "   feature 10 = 1:\n",
            "      feature 8 = 0:\n",
            "         feature 2 = 0:\n",
            "            feature 9 = 0:\n",
            "               feature 12 = 0:\n",
            "                  prediction: 0\n",
            "               feature 12 = 1:\n",
            "                  feature 7 = 0:\n",
            "                     feature 1 = 0:\n",
            "                        feature 0 = 0:\n",
            "                           feature 15 = 0:\n",
            "                              prediction: 0\n",
            "                           feature 15 = 1:\n",
            "                              prediction: 1\n",
            "                        feature 0 = 1:\n",
            "                           prediction: 1\n",
            "                     feature 1 = 1:\n",
            "                        feature 0 = 0:\n",
            "                           prediction: 1\n",
            "                        feature 0 = 1:\n",
            "                           feature 11 = 0:\n",
            "                              feature 4 = 1:\n",
            "                                 feature 5 = 1:\n",
            "                                    feature 6 = 0:\n",
            "                                       feature 13 = 1:\n",
            "                                          feature 14 = 0:\n",
            "                                             feature 15 = 1:\n",
            "                                                prediction: 1\n",
            "                           feature 11 = 1:\n",
            "                              prediction: 1\n",
            "                  feature 7 = 1:\n",
            "                     prediction: 1\n",
            "            feature 9 = 1:\n",
            "               prediction: 1\n",
            "         feature 2 = 1:\n",
            "            feature 1 = 0:\n",
            "               prediction: 1\n",
            "            feature 1 = 1:\n",
            "               prediction: 0\n",
            "      feature 8 = 1:\n",
            "         prediction: 0\n",
            "test acc:  [0.8863636363636364, 0.9090909090909091, 0.9318181818181818, 0.9545454545454546, 0.9090909090909091, 0.9767441860465116, 0.9069767441860465, 0.9767441860465116, 0.9302325581395349, 0.9302325581395349]\n",
            "avg acc:  0.931183932346723\n"
          ]
        }
      ],
      "source": [
        "# Used 10-fold CV on Voting Dataset\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff --output voting.arff\n",
        "\n",
        "voting_data = arff.loadarff(\"voting.arff\")\n",
        "\n",
        "voting_df = pd.DataFrame(voting_data[0])\n",
        "voting_df = convertBytestoString(voting_df)\n",
        "\n",
        "voting_df = voting_df.replace(voting_dict)\n",
        "\n",
        "voting_np = handle_missing(voting_df)\n",
        "voting_np = voting_np.astype(int)\n",
        "v_counts = np.array(get_counts(pd.DataFrame(voting_np)))\n",
        "\n",
        "k_splits = 10\n",
        "cv_tracter = K_Fold_CV(voting_np, k_splits)\n",
        "cv_accs = []\n",
        "\n",
        "# starting CV loop \n",
        "for i in range(k_splits):\n",
        "    t_set, v_set = cv_tracter.get_datasets_at_index(i)\n",
        "    tmp_clf = DTClassifier(counts=v_counts)\n",
        "    tmp_clf.split_and_fit(t_set)\n",
        "    if i == 0:\n",
        "        tmp_clf.print_tree()\n",
        "    acc = tmp_clf.score(v_set[:,0:], v_set[:,len(v_counts)-1:])\n",
        "    cv_accs.append(acc)\n",
        "\n",
        "# Report Training and Test Classification Accuracies\n",
        "print(\"test acc: \", cv_accs)\n",
        "\n",
        "# Report Average Test Accuracy\n",
        "avg_acc = sum(cv_accs)/k_splits\n",
        "print(\"avg acc: \", avg_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBzLv0l-CmL2"
      },
      "source": [
        "## 2.4 Discuss Your Results\n",
        "\n",
        "- Summarize your results from both datasets, and discuss what you observed. \n",
        "- A fully expanded tree will often get 100% accuracy on the training set. Why does this happen and in what cases might it not?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLJq3SCcCmL2"
      },
      "source": [
        "Discuss your results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCi8FgyzCmL2"
      },
      "source": [
        "## 3. (15%) For each of the two problems above, summarize in English what the decision tree has learned (i.e. look at the induced tree and describe what rules it has discovered to try to solve each task). \n",
        "- If the tree is very large you can just discuss a few of the more shallow attribute combinations and the most important decisions made high in the tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUO9beTOCmL2"
      },
      "source": [
        "## 3.1 Discuss what the decision tree induced on the cars dataset has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxJsPkQkCmL3"
      },
      "source": [
        "Discussion Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip5zeIIFCmL3"
      },
      "source": [
        "## 3.2 Discuss what the decision tree induced on the voting dataset has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlUsN_SfCmL3"
      },
      "source": [
        "Discussion Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGLflx_FCmL3"
      },
      "source": [
        "## 3.3 How did you handle unknown attributes in the voting problem? Why did you choose this approach? (Do not use the approach of just throwing out data with unknown attributes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YmvT4T7CmL3"
      },
      "source": [
        "Discuss how you handled unknown attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bszs7X95CmL3"
      },
      "source": [
        "## 4.1 (10%) Use SciKit Learn's decision tree on the voting dataset and compare your results. Try different parameters and report what parameters perform the best on the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EptPkscCmL3"
      },
      "source": [
        "### 4.1.1 SK Learn on Voting Dataset\n",
        "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "xpvOixmjCmL4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set score 0.9954022988505747\n",
            "cross validation score\n",
            " [0.90909091 0.90909091 0.95454545 0.90909091 1.         0.97674419\n",
            " 0.95348837 0.86046512 0.93023256 0.97674419]\n",
            "avg acc:  0.9379492600422832\n"
          ]
        }
      ],
      "source": [
        "# Use SK Learn's Decision Tree to learn the voting dataset\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X = voting_np[:,0:len(v_counts)-1]\n",
        "y = voting_np[:,len(v_counts)-1:len(v_counts)]\n",
        "clf = tree.DecisionTreeClassifier(max_features=\"sqrt\")\n",
        "clf = clf.fit(X, y)\n",
        "\n",
        "# training set accuracy\n",
        "print('training set score', clf.score(X, y))\n",
        "\n",
        "# validation set accuracy\n",
        "cv_accs = cross_val_score(clf, X, y, cv=10)\n",
        "print('cross validation score\\n', cv_accs)\n",
        "\n",
        "avg_acc = sum(cv_accs)/10\n",
        "print(\"avg acc: \", avg_acc)\n",
        "\n",
        "# Explore different parameters\n",
        "\n",
        "# Report results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByCPKa5vCmL4"
      },
      "source": [
        "Discuss results & compare to your method's results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taIOEaWCCmL4"
      },
      "source": [
        "## 4.2 (10%) Choose a data set of your choice (not already used in this or previous labs) and use the SK decision tree to learn it. Experiment with different hyper-parameters to try to get the best results possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "MU2XN8cOCmL4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set score 0.9523809523809523\n",
            "cross validation score\n",
            " [0.   0.4  0.5  0.75 0.5  0.   0.5  0.   0.   0.  ]\n",
            "avg acc:  0.265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Alex\\OneDrive\\Desktop\\Fall2021\\CS472\\CS472-Labs\\Lab3\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Use SciKit Learn's Decision Tree on a new dataset\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# !curl -s https://axon.cs.byu.edu/data/statlib/nominal/analcatdata_birthday.arff --output data.arff\n",
        "!curl -s https://axon.cs.byu.edu/data/statlib/nominal/analcatdata_fraud.arff --output data.arff\n",
        "\n",
        "data = arff.loadarff('data.arff')\n",
        "df = pd.DataFrame(data[0])\n",
        "df = convertBytestoString(df)\n",
        "\n",
        "# df = df.drop('Births', 1)\n",
        "\n",
        "# assigning ids to values\n",
        "df['AC1'] = pd.to_numeric(df['AC1'], errors='coerce')\n",
        "df['AC9'] = pd.to_numeric(df['AC9'], errors='coerce')\n",
        "df['CL7'] = pd.to_numeric(df['CL7'], errors='coerce')\n",
        "df['CL11'] = pd.to_numeric(df['CL11'], errors='coerce')\n",
        "df['IJ2'] = pd.to_numeric(df['IJ2'], errors='coerce')\n",
        "df['IJ3'] = pd.to_numeric(df['IJ3'], errors='coerce')\n",
        "df['IJ4'] = pd.to_numeric(df['IJ4'], errors='coerce')\n",
        "df['IJ6'] = pd.to_numeric(df['IJ6'], errors='coerce')\n",
        "df['IJ12'] = pd.to_numeric(df['Fraud'], errors='coerce')\n",
        "df['Total'] = pd.to_numeric(df['Total'], errors='coerce')\n",
        "\n",
        "data_np = handle_missing(df)\n",
        "data_np = data_np.astype(int)\n",
        "data_counts = np.array(get_counts(pd.DataFrame(data_np)))\n",
        "\n",
        "X = data_np[:,0:len(data_counts)-1]\n",
        "y = data_np[:,len(data_counts)-1:len(data_counts)]\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", splitter='random')\n",
        "clf = clf.fit(X, y)\n",
        "print('training set score', clf.score(X, y))\n",
        "\n",
        "# validation set accuracy\n",
        "cv_accs = cross_val_score(clf, X, y, cv=10)\n",
        "print('cross validation score\\n', cv_accs)\n",
        "\n",
        "avg_acc = sum(cv_accs)/10\n",
        "print(\"avg acc: \", avg_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbAEEv1RCmL4"
      },
      "source": [
        "## 5. (5%) Visualize sklearn's decision tree for your chosen data set (using export_graphviz or another tool) and discuss what you find. If your tree is too deep to reasonably fit on one page, show only the first several levels (e.g. top 5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "dXH8-BD2CmL4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Text(241.23196022727274, 205.35999999999999, 'X[4] <= 0.472\\nentropy = 2.183\\nsamples = 42\\nvalue = [24, 4, 4, 3, 1, 1, 2, 2, 1]'),\n",
              " Text(185.709375, 181.2, 'X[8] <= 0.339\\nentropy = 2.503\\nsamples = 28\\nvalue = [12, 4, 3, 3, 0, 1, 2, 2, 1]'),\n",
              " Text(139.34147727272727, 157.04, 'X[7] <= 0.343\\nentropy = 2.483\\nsamples = 16\\nvalue = [6, 1, 3, 1, 0, 0, 2, 2, 1]'),\n",
              " Text(111.28295454545454, 132.88, 'X[3] <= 0.195\\nentropy = 2.412\\nsamples = 13\\nvalue = [4, 0, 3, 1, 0, 0, 2, 2, 1]'),\n",
              " Text(77.99318181818182, 108.72, 'X[6] <= 0.428\\nentropy = 2.322\\nsamples = 10\\nvalue = [4, 0, 1, 1, 0, 0, 1, 2, 1]'),\n",
              " Text(49.45909090909091, 84.56, 'X[5] <= 0.441\\nentropy = 1.842\\nsamples = 7\\nvalue = [3, 0, 0, 0, 0, 0, 1, 2, 1]'),\n",
              " Text(30.436363636363637, 60.400000000000006, 'X[0] <= 0.36\\nentropy = 1.371\\nsamples = 5\\nvalue = [3, 0, 0, 0, 0, 0, 0, 1, 1]'),\n",
              " Text(15.218181818181819, 36.24000000000001, 'X[2] <= 0.305\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
              " Text(7.609090909090909, 12.079999999999984, 'entropy = 1.0\\nsamples = 2\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
              " Text(22.827272727272728, 12.079999999999984, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(45.654545454545456, 36.24000000000001, 'X[1] <= 0.036\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
              " Text(38.04545454545455, 12.079999999999984, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
              " Text(53.263636363636365, 12.079999999999984, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(68.48181818181818, 60.400000000000006, 'X[0] <= 0.731\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 1, 1, 0]'),\n",
              " Text(60.872727272727275, 36.24000000000001, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
              " Text(76.0909090909091, 36.24000000000001, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0]'),\n",
              " Text(106.52727272727273, 84.56, 'X[9] <= 0.707\\nentropy = 1.585\\nsamples = 3\\nvalue = [1, 0, 1, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(98.91818181818182, 60.400000000000006, 'X[5] <= 0.036\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 1, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(91.30909090909091, 36.24000000000001, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(106.52727272727273, 36.24000000000001, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(114.13636363636364, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(144.57272727272726, 108.72, 'X[0] <= 0.256\\nentropy = 0.918\\nsamples = 3\\nvalue = [0, 0, 2, 0, 0, 0, 1, 0, 0]'),\n",
              " Text(136.96363636363637, 84.56, 'X[5] <= 0.979\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 0, 1, 0, 0, 0, 1, 0, 0]'),\n",
              " Text(129.35454545454547, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0]'),\n",
              " Text(144.57272727272726, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(152.1818181818182, 84.56, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(167.4, 132.88, 'X[3] <= 0.534\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(159.7909090909091, 108.72, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(175.0090909090909, 108.72, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(232.07727272727274, 157.04, 'X[9] <= 0.823\\nentropy = 1.73\\nsamples = 12\\nvalue = [6, 3, 0, 2, 0, 1, 0, 0, 0]'),\n",
              " Text(205.44545454545454, 132.88, 'X[3] <= 0.301\\nentropy = 1.406\\nsamples = 8\\nvalue = [4, 3, 0, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(190.22727272727275, 108.72, 'X[0] <= 0.775\\nentropy = 1.5\\nsamples = 4\\nvalue = [1, 2, 0, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(182.61818181818182, 84.56, 'X[5] <= 0.877\\nentropy = 1.0\\nsamples = 2\\nvalue = [0, 1, 0, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(175.0090909090909, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(190.22727272727275, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(197.83636363636364, 84.56, 'entropy = 1.0\\nsamples = 2\\nvalue = [1, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(220.66363636363639, 108.72, 'X[0] <= 0.19\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(213.05454545454546, 84.56, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(228.27272727272728, 84.56, 'X[5] <= 0.088\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(220.66363636363639, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(235.88181818181818, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(258.70909090909095, 132.88, 'X[6] <= 0.472\\nentropy = 1.5\\nsamples = 4\\nvalue = [2, 0, 0, 1, 0, 1, 0, 0, 0]'),\n",
              " Text(251.10000000000002, 108.72, 'X[0] <= 0.522\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
              " Text(243.4909090909091, 84.56, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(258.70909090909095, 84.56, 'X[1] <= 0.18\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
              " Text(251.10000000000002, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
              " Text(266.3181818181818, 60.400000000000006, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(266.3181818181818, 108.72, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
              " Text(296.75454545454545, 181.2, 'X[8] <= 0.059\\nentropy = 0.735\\nsamples = 14\\nvalue = [12, 0, 1, 0, 1, 0, 0, 0, 0]'),\n",
              " Text(281.53636363636366, 157.04, 'X[0] <= 0.464\\nentropy = 0.469\\nsamples = 10\\nvalue = [9, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
              " Text(273.92727272727274, 132.88, 'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(289.1454545454545, 132.88, 'X[3] <= 0.712\\nentropy = 0.722\\nsamples = 5\\nvalue = [4, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
              " Text(281.53636363636366, 108.72, 'X[9] <= 0.493\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
              " Text(273.92727272727274, 84.56, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(289.1454545454545, 84.56, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
              " Text(296.75454545454545, 108.72, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(311.9727272727273, 157.04, 'X[0] <= 0.667\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(304.3636363636364, 132.88, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(319.5818181818182, 132.88, 'X[3] <= 0.517\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(311.9727272727273, 108.72, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
              " Text(327.1909090909091, 108.72, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0]')]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAADnCAYAAAC5W1UtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA19ElEQVR4nO29e3xcV3Xo/12yZM3YiSONX3Jsx2NiJXaCie34hWOcAEnIO0oLocCPx+U2pDRA0tILvYWUQttwS+nl1ZbnjzakFAqlFnkTINiKlYQQEmMbDIlrW4qRJeNEY8eRRraSdf/YR8pIOjMazZwz54y8vp+PPpZH5+y19j5n1lln7b3XElXFMAzDqCw1UStgGIZxMmLG1zAMIwLM+BqGYUSAGV/DMIwIMONrGIYRAWZ8DcMwIsCMr2EYRgSY8TUMw4gAM76GYRgRYMbXMAwjAsz4GoZhRIAZX8MwjAgw42sYhhEBZnwNwzAiwIyvYVQxyWSyW0Q0iJ9kMtkddX9OJsTy+RpG9SIiGtR3WERQVQmkMWNcaqNWwDCM4NmyZQunnnoq8+bNY3BwEFUlk8mQSCTo6+tj5cqVUat40mNhB8OYZLS1tXHs2DGOHDnC9u3bSSQSw4a3u7ubadOmRa2igYUdDKOqsbBD9WJhB8OYhGzbto2enh5mz56NqpJOp2loaGD//v0sX76cmhp76Y0aM76GUaWIiABs3bqVdDrNgQMHaG5u5sSJEySTSVavXk1XVxeJRIJUKsXevXuZOnUqTz/9NAsWLGDv3r0MDg4ybdo0EolE1N056bCwg2FUGSJSD/wBcEsikViezWanBNFuIpEYyGaz1wA/DCyWYeTFjK9hVAki0gS8F7gR2A58DviBqr4UQNtJ4K3AzcAU4PPAHaraV27bhj9mfA0j5ojI+TijeDXwbeDzqro7JFkCXATcAmwA/n/gn1T1mTDkncxY1N0wYoiI1IrIG0XkIWAzsAs4U1XfG5bhBVDHT1T1WmA9UA/8QkT+Q0Q2DMWZjfIxz9cwYoSINAJ/CLwPeAYXWtisqoMR6jQDeBfwAaAX+CzwXVU9HpVOkwEzvoYRA0RkKc64vQW4G/icqj4erVYjEZEpwBW4EMg5wBeBL6vqoUgVq1Is7GAYESEiNSJymYjcD2wFDgPnqOrb42Z4AVT1RVW9S1UvBt4AnAH8RkS+LiLnRaxe1WGer2FUGBGZDrwD50Fmca/x31bVbJR6lYKIzALeA9wEPIULk9ylqi9GqlgVYMbXMCqEiCzCGal3Aw/hjG7bZFhTKyJ1wBtxqyRmA18Avq6qR6LUK85Y2MEwQkQcG0XkP4EncLtK16rqdaq6dTIYXgBVPaGq31LVdbj1wmuBfSLyeRFpjli9WGKer2GEgLcL7c240MIM3Ov47ar6fKSKVRARWQD8MXAD8ChuDH48WR445WLG1zACRETmAn/k/ezChRbuC2IXWrUiItOAt+EeROCM8L+pan90WkWPGV/DCAARWYkzLtcC38HtQvtltFrFC2+Dxutx47QO+Brwz6p6IFLFIsJivoZRIiIyRUR+T0S2AncCvwaWqOqNZnjH4u2e+5GqXg1cAJwC7BCRb4nI+ojVqzjm+RrGBBGRVbh47vXAQdxr9H+p6olIFatCROQ03OqP9wO/A+7HbTB5LlLFKoAZX8OYICKyA2gCrlLVx6LWZzLg7Z67CmgFvqCqH4hWo/Ax42sYhhEBFvM1Jj3JZLJbRLTcn2Qy2R11X042JvO1M8/XmPQEVWTSCkxWnsl87ayGm3FS0t7ezuDgIM3NzQwODqKqZDIZslmXXmHdunURa2jkw+/adXe/7NhWy7Uzz9eY9Ph5T729vSQSCU6cOMGMGTOKbSd23tNkZzJfO/N8jZOSnTt3kvulXrZsGQcPHiSbzXL8+HHmzJnD2WefHaGGRj78rl1XVxfJZJJMJkNTUxOLFi2KUMPisAk346SkpqaGw4cPM1QVp7+/n3Q6zfHjx1m/fr0Z3hjjd+2WLFlCNptlzZo1VWF4wTxf4ySho6ODAwcO0NzczIkTJ3jxxRdZv349nZ2d1NbWkkql2Lt3L7W1tezdu5cFCxawd+9eBgcHmTZtGolEIuounLQUunaNjY0APPPMM0ydOpUdO3awePFidu3aRU1NDalUKrbXzmK+xqRGRM5IJBK/zmazyXLbSiQSh/r7++cGoZdRHMlksjubzZY95olEoqe/v78pCJ2CwsIOxqREROaKyOeAJ7PZ7GeAlKpKKT+4emXfzWazL4rI+7x0kUYF6O/vbxp1LaYADwM3+VynrwJf9LuGcTO8YMbXmGSISEpEPgn8ClBcTbSPqGpvqW2q6m5VvR64ErgcV7fs3SJiYbvKcwMgwJd8/vZhoEVEXl1ZlUrDjK8xKRCRU0Xko7g6YrOAlap6i6r2BCVDVZ9U1StxuWnfAfxSRN4sIvY9qgAiMg/4G+A9fvmRvQfsnwBfEZGpldZvothNY1Q1IpIQkT8BnsaFB16tqjeoamdYMlW1HXgt8D7gz4AnROQqGZp+N8Lic8BXVXVXgWO+AzyDuy6xxibcjKrEK9j4P4BbgZ8Dt6rqzgj0EFwC9b8Gngc+oqo/qbQekx0RuRJnfJePVwFDRNLA48B6Vd1TAfVKwoyvUVV4qQf/APg4sA/4qKr+NFqthvV6M/AJYD/OCEeu12RARE7BlWS6QVV/WOQ5H8TF5y+Ja804M75GVeB5mC04D/MoMfUwPY/8XcBf4qoV36qqOyJVqsoRkX8AZqvqOyZwTi3wGPBZVf1GaMqVgRlfI9Z4RvcS3ETLVOAjwL1x9WaGEJEErojmnwMPAh9T1aej1ar68KqG3Ae8UlV/N8FzVwN3e+ceDkO/crAJNyO2iMgFwE+AzwOfBlap6j1xN7wAqppV1c8CS4BfAg+LyFdF5IxoNasePO/1q8CHJ2p4AVT1ceDbuHsndpjxNWKHiKwSkXuBbwK34zyX71Rj+XVVPaaqfwuchatR9qSIfM4rMW8U5v3AEdw9UCq3Aq8VkdcFo1JwmPE1YoOILBOR7+JeFe8FzlbVf1HVwYhVKxtV7VXVv8AthwP4lYjcJiKNUeoVV7w3hI8AN5bzpqOqz+OWBH7JCwXFBjO+RuSIyGIR+VdgK/AzXPn1f1TVgWg1Cx5V7VHVm4GVwBzgKRH5iDejbzAc5/9n3GRZ2XFyVb0L2IEz5rHBjK8RGSJyuoj8E25NZgfQrKqfUtW+iFULHVXtVNU/BC4AzgX2iMgtcfPOIuI/gDXApwJs8wPAh0TkjwNssyzM+BoVR0Q2iMgdwE6gH1iqqh9T1SMRq1ZxVPUpVX0rcClu19zTIvIVEWmIVrNIeQ74jKoeD6pBVe0CvkWM0ujaUjOj4ojIFuBsYLWq/jZidWKFiKwHHgH+VFU/E7U+RniY8TUMw4gACzsYRZNMJrtFRMv5SSaT3eNLMiCY8Y7DmEfVj7iPn3m+RtGITyXZEtqIXRXZuBLEeHvtRDrmUfUj7uMXm+CzUZ20t7cza9YsTj31VAYHB1FVMpkM2WyWmpoa1qxZE7WKk4r29nZqa2tZuHDh8Hh3dHRQX19fVePd3t6OiJBOp0fcNwMDA9TV1bFy5cqKye3udo5tKpWiubk5FLl+mOdrFI2fJ9HT08OsWbN44YUXmDFjRjFtmOdbJH7j3dvbSyKR4MSJE0WNt9dO7DzfSvQj7uNnnq9RMm1tbeTe3MuWLePgwYMAZDIZmpqarAR7wOzcuXPMmPf2ugpJR48erRrP168fXV1diAgDAwOkUimWLFlSMblHjhwhlUoxY8aMipWetwk3o2Q2bdrElClTOHz4MCLC7t27aWho4Mwzz2Tq1Km84hWviFrFSYffmCcSCebPn8/UqVN56aXqSH9RU1Mz3AeA/v5+lixZQn9/P+edd14ohhf8x6+xsZEVK1agqhUzvGBhB2MC2IRbZYn7hNEE5NuEmw8WdjCKpq6u7jkRSZXTRiKRCKyg5WQnkUj0BJH9LOoxD+K+gYn3I+7jZ2EHo2iOHz8+E2gAHsBlHTtDVSXfD3Aa8APgfu/3VH9/f1NU+lcb/f39Td44TgfuwY35gnHG/PdxqStf7/0/8jE/fvz4zFE6prx/3wT8wPv9Lbgk+X59SqmqTLQfQ+M3ui1gCvBbYJn3ezdwVj7ZYY2fGV+jaLw0f9uAPcC1qvpMoeNV9ShwFS5pzkOAZe6aICIyE/gx8CzQMt52bFX9L+B64Nsi8iZ15dRjRY5O1wGt3u/3Aq8RkTFLEILsg9fWauCYqv7ayxH9fVyJqkK6Bo4ZX6MoROR8XM6BrwPvKzbHrnfce4E7cNUcVoWn5eRCRBbiHloPAe9S1RPFnKeqW3Cllz4bpyxeuYjIVFyBy+/D8IO6HbisAuJbgM05/2/FPQgqihlfY1xE5Fpc6OAmVf3MRGcx1PFp4BbgByJyVQhqTipE5FycMfqaqn6ohDH/BbARuEVEPiFDywriw0XAr1X1YM5nm6mMEbyOkcb3QWCZiMyrgOxhzPgaBRGRm3GJra9Q1dZy2lLV7+HCEF8RkfcHoN6kREQ24AzCX6jq/y21HVXdhzPAVwBfFlcTLS7khhyGuBO4TETqwxIqIkuBGbgc0gB4qSvvA64JS64fZnwNX0SkVkS+ALwHuEBVfxZEu6r6U1wC8feKq2U2JYh2JwveW8H3gXeq6r+V256qHsLlCV4MfFdEkuW2WS4iUgNcy0jvE1XtBn6F0zcsWoBWHVsPsJUKhx7M+BpjEFfSphVYijO8+4Ns3/PINgCvBP5LRKYH2X61IiLvAr4GXKWq9wfVrro6ZlcCWeB+iT5R+1rgOfUvERR26MHP4wbn+V4gIqeFKHsEZnyNEYjIfKANt/zmClXNhCHHa/dyXNWCNhE5PQw51YA4Pgz8FXCR93YQKN6r9duAXxD9eOczgHifX+t5x4Hi3dvNwJbRf/MeUG24EE1FMONrDCMi5+FWNHwHuKHY2fVS8QzCu4HvAY+IyPIw5cURz8j8X+D/w71l/DosWd6r9s24cjrtInJWWLLy4U38jZ7wGkZV9+DWKa8PQfy1wD0F7utKTfgBZnwNDxG5HPgh8Geq+n8C2ZdZBN5KiNuAPwd+LCJvqITcOOAtt7oDt+5003hreIPAG+9PAn8NbBWRSmfiWQYkgCcKHLOZPOtuyySv0fe4E7hUKlTE1IyvgYi8F7d+t0VVvxOFDqr6LeD3gNtF5D1R6FBJvLj6XbiNJ5dWejOEqn4duBG4R0QuraDo63ATXoUe7q3AdUEujxORRmAdbselL97k5A7g9UHJLYQZ35MYEakRkU/jXkU3qurDUeqjqtuA1wB/JiKfCiPuFwdEZDZuKdkzwO+ran8UeqjqnbgH3h0i8tYKiR3P+wR4EpgKnBug3CuBn6jqC+Mc10qFQg+T8uY2xkdEpgH/iXvl3aCq/x2xSgB4M+CvxsX8vhOHpVFBIiJp3BbtB3Bx9aJ2CoaF98B7PfB3InJzmJsxvB17adyOvUI6KfA88B8Bii/G6AM8DPzPSqx6MON7EiIipwIvAC8Bb1DV5yJWaQSq+ixue2wN0Oc9KKoeb4v2PtyutY9WKq4+Hqq6C7cZ42+Ax0IUdSuwtcgHzs3AF4IQKiILcCtr7i7i8J8BX8IZ/1CxfL4nId6Ewqdxk2vZqPXJh+f1/j3wp97KiKrGS6v4buAf4mJ4c/Em35Z48fcw2u8C7lPV/xlG+wXkvh34BlATp3E342sYhhEBFnaoUpLJZLeIaLk/yWSyO+q+5COufYyrXuUQVZ+iHMuor6N5vlWKxLxEShDEtY9x1ascoupTlGMZ9XWMU5YjIwDa29upra1l4cKFDA4OoqpkMhkymQyJRIJ169ZFrWJZ+PWvu7ubbDbLjBkzWLlyZWz0Ghr3uXPnsnTp0kj0Kof29nbq6upYsGDBiD4lEgn6+vqGxzqdTtPR0TF8noiUZdH8xjKbzXLo0CFOOeWUEdc4V3a5cvPJHrq/Tj/9dJqbm8sVMYx5vlVKvqd2T08Ps2bN4oUXXmDGjDFFAfzaiY33NRq/Pk60f147oXu+cdCrHPz61NvbSyKR4MSJEwX75PVjzOdtbW2+ZdoHBgZYsWIF9fX1vucWKzef7GLk5jt3IrJH6WCe78mM30138KDLVX3s2DHmz59POp2OSLvyKfSlOn78OHPnzuWssyqersBXr97eXrLZLJlMhjPOOIPFixdXXK9y2blzp2+/MpkMa9euHS77no+amhp6enqYPXs2qkp/fz+LFy9mz549wwawWLldXV0kk0n6+vpobGwseB/nk9vZ2VlQ7nh9zjXcQWCeb5WSz/Pdtm3biBsvnU4zc+ZMdu7cyZo1a6itrR3dTmy8r9H49dGvf42NjezZs4dVq/wrFFXC88037jt27GDDhg0V0ascJnI/NTQ00NHRwate9aqhc3093yLl+p6bT+6ePXs4//zzxz2/VNn55HZ2drJ8uX/ep1KvoxnfKiXqyYJKENc+xlWvciinT6NjvhOlnLFsamqip6f0yu424WZMmEQi0SMic4NoJwh9wqCuru45cRsTyiLoPsZVr3IIsk/9/f1NItJYTLIgb7lXIPfxROQCTJ069dkor6Ot861S+vv7m4AULj/DDuAcIKWqUugHt420A7d1c57XTiw5fvz4zFG6p7x/bwLu8H7/O+C2PH1NqaoE3UcfveYBTwPXjvr8LFzJ93Ql9CqHAmN9Bi7h/VRczo1fFjPWxRrA/v7+pnxt4fI8v9v7/THgknyyJyp3nD6/BviF9/tbgbvDuL/M+FYpInIBLvtTF7BOVXcXc+OpajuwEmjCVRI+J1xNgyOnf7mVEFrJk4VqIl/EMrkJ2KkuS1iu/KeBzwD/JDmzUxXUq2RydLwWZ3xO4AxgSkTGrLcKsk+q2itua/klvJyPoZU8OX6Dkp3n/roXuFBcPpRA5ZrxrTJEZIqI3IrzCt6vqjfrBPMzeDfNm3Heb5uI3JBrHOKMjM3L+hjQICJnR6TPOcAfAR/Ic8jf4zJ5vbFSOgXMsCFSVwnj+1Qm5eLFwJOq+jvv/5uBFgk5zaj3PRjOgKaqR3CZzi4LWpYZ3ypCXEq+B3HVXVep6l2ltqWOr+Fesd6HS9/YGIymoTIiL6tnEFoJp/JBQTxD8BXgY5qnCoW6hEA3Ap+T6AtXTggRmQmsYWQC8lYqM9YjUkCqK6/0PC4FapgsBwQXyhsilMoaZnyrBBG5DngcuB8X++oKol1V3Y3zJLuAJ71wRpzxy8vaSoXLfnv8ITAFl4IwL16o5/vA/6mEUgFyJfBjVe3L+ewnwDIRmReWUBGpBa7GjVkurYR/na8DNo9a+nEncLm4sk/Boar2E+MfIAn8M7AXWB+yrKtxVYv/EpgSdd/zjMURYNaoz6fiJoVOr6AuTcAhYHmRxzcAv8UVyYx8LIvUeTPwTp/P/x24MUS5FwJP+Hy+Fvh1yH1+EldPb/TnD+PKPQUmyzzfGCMir8Qld24AVqrqo2HKUxfGWIW7+R/0whxx4hLcl/Jw7ofqXu3vxU0OVYrP4pKi7yzmYFXNALcAXwncgwoBcQnsX4+rMzeasKv85qs68ThwqoiEkihDXJWR+UC7z59bCbjPZnxjiDjei3vF+3vgbeoC/6GjLpxxKS688bgX7ogLhUrBtFKh0IOIXIGLPf71BE/9T1wliw8FrlTwXAr8TP2rnNwPbJAQSu14E14tvLzaYBh9Ob4f1nVuAe5U1Rd9/rYZuDbQCb+oX23sZ8zrzUzvQj8BnBWxLutx4Y4vAsmIdakFfgcsyvP3U4CjQEPIekwH9gMXl3j+IuBw1Ne2CD1vx62myff3u4G3hCB3JbAHb/etz98vBn4aUp+3AlcV+PsuAgz9mecbI0TkQlzMaS/walV9Kkp91IU5VgKnAT/zwiBRsRHoVFXffayqegzYgpskCpOPA22q+qNSTvb0/1vgS3Fd3udNeF2Fj/eZQ1ihhxbGTnjlshVoFpH5QQoVV1F6BVDourYSYJ/N+MYAEakVkU8A3wLeo6ofVNWBqPWC4XWOb8OFP34iIu+NyGjkLnzPRyshLoMSkVXA24EPltnUF3APtHeUrVQ4bAL2quozBY65C7hUXD3AICl4ndVt9riH4OP7VwMPaOE185uB64K6/834RoyILMI9zdfh1u7eH7FKY1DH7cAFuOVV/+WtAa0IOXHA8Up/3wVcIiGUm/e8wa8AH9aXF/6XhLrqvTcAn/I8rrgxbpl1VT2EWwv7+qCEisiZwBxgvInlMLzuYkrLPwEkgGVBCDTjGyEi8ibcaobNwOWqGpuaXn54YZANuLDIk16YpBKsBI4Dvyx0kGcUt+PigkHzVeAYLhZaNqr6BPBvwPdEZEoQbQaBt432JlwIZzyCNoJ3AM+o/4RXLj8A1gW1Kcjr84U4jzovXiiklYD6bMY3AkTkDBF5HPgkcKWqflrdTG7sUdUBVf0g8B7gWyLSLiLTQxZ7A24Wupj8f98H/kcIOrwL+HKROhTLZ3E7DAPxpALiBM7zfKKIY1txr+FBXf+f4t4uCqJud2MbwV3ntwOPaXErilqBtwTxwLR8vhEgIn+M2+00X1Wfj1qfUhGRxcBu4ApVfTBEOQr8lap+vIhjr8EZ4JogDaUElcS3Qu1WAi8c9BJwg7qt6pWU/UNcQqniav0Ubusw8JSq+me9H3lsCpepbrmq7ipLbpVed+MkQkSagJ5ijZSIzFPVgyGrZQBeLt5DlX6AeOttZ3mx53LbmgM8W0S4Y+j4QO4vM76GYRgRYDHfEvCy72u5P8lkMtYTbMVQjWMRpc5RyT4Z+xx3zPMtAb8wXXt7O3V1dSxYsIDBwUFUlUwmQzabJZvNcuGFYxcGSIxqeJVKvrEQEdLp9PBYdHd3Dx3P2rVrgfJqf9XX1zMwUPpSaL/73k/vbDbLoUOHmDp1KuvWrRtxfCnXr9jxymQyZDIZ3/umFNkTuU7ZbJYZM2awcuXK4WODrtPW3t5ObW0tCxcuHPN9qampYc2aNWXLLeceKff+GmKovJHf38z4loDfjdzb20sikeDEiRPMmFHcHMBkNb7FjoX4VI8tVB4+t3S337nFnp/v3Ilew6CMbyXunXLlBn2tqvUeKVb2KP19r5MV0AyInTt3jrkoBw8eHH6Sz549m3Q6HZ2CFcRvLIZu0OPHjzN37lzOOuss33M3bdo0onz37t27h8uw79u3j6VLCye0qqmpGVH6u7+/n8WLF9PZ2Tnmi1GM3r29vWQyGdauXYuEtLEv33gdOXKEVCrFjBkzWLRoUcXkFnOdwH+slyxZwoEDB0oa63LvkYaGBp566qm8Jd6D0Lucc0djnm8J+HkRwIgbQlWHjcauXbtYuXLlhJ6K1YLfWPiNQ2NjI3v27GHVqlW55/p6F0XKDfxcP70bGhro6OjgVa96Vb52yvZ8KyF7InIrca2q9R4psR3f62TGtwTy3cgltDMpje8EzmX//v0cOHCA5uZmTpw4QWdnJ42NjSSTSfr6+shkMiSTSRYvXsyuXbuoqakhlUqxdOlStmzZQjqdHnO+iDBv3ry852/YsCH0L1aBcyK5d8qVm+9a5Y41wMDAgO+1CvoeGe8aF3OPnHfeeRw6dIiuri7fe8Tv3KNHj3LkyJFxZScSCQ4cOMDGjRst7BA0W7duHfei9vX1jbgZU6kU06ZNo7a2lv3790fdhcDo6OjwHYdFixYNTxzl3qCpVIqamhoWLFhQciimvr6eiy66qGSd/a7f0aNHyWazpFKpgsZk6ItVKqPHa+/evcyePXvcB86Q3AsuKK3SU6F71s+YdHV1kclkSKVSZV0rvz77yU2lUjQ1NY3oc1T3SLn31xCJRKIn39/M8y2BZDLZnc1m55bbTqGZ0GqhGsciSp2jkn0y9jnumPEtEXElw+8GvomrXjtubgYRuRyXmOUWVf33kFWMFHEpMpOq+r9E5HRcIuomdSV/Yom4GbWngD9Q1Z+LyJ8A56rqH1ZA9k247bLvEJcr4SAucXxvyHITuLp9zar6OxH5HPA7Vf2bMOV6sv8e6FfVvxSRVwCP4OrwFbXTrNqxTRYlICKX4rI+fUxVby3G8AKo6n3A64DbROTjEtb0eTxowUvRp6400W+AiyLUpxjOwRXjHEoqsxm4RiqTdWw4paG6xDEP4hKah83FwC/05TSZm6lAaXjv3s/t817cQ+DVYcuOC2Z8J4i42mq3A7+vqndM9Hx1yTjW4Wpk/buEkHs2asQ/L2tFvtRlch3QOjQzpar7cRWHx024Ug7iUiOuxaVKHKKVytSka2FkHtttwCIROSNkuefi5py253wWdmHOWGHGt0hEZIr3SvYBYKOqPlRqW6rag/OAwVUJLjseFjOuA74/6o2gFWiRIAsQBo9fFYVWwjcIVwEPqmpfzmd3AReLqyIcCp5Hfw05fVaX6P1uwn9QjnjQebTi7pHJ/EY4TJy/CLFBXLLlO3FP61er6n+X26aq9gNvBR4AHpVo66MFTQujjJi6ROy9wJoI9BkXz9NbBIx+qAZaOiYPLYyqoqCqzwI/By4JUe4G4Leeh59LK+E/cPwqR/wCZ5MK75KYJJjxHQdxZX7agQO4ahOZoNpWx8eAj+I84MuDajsqPC/+XFzMcjStxPe1sgW42/P8ctkJKDB2p0MAeGGni3He5mjCfg3PVy/tAWCVhFQqyvtOLcR9r4bxvOCTJvRgxrcAIrIONwP7L8AfqSveFziq+k3cDfd1EXl/GDIqyLXA/epfALQSXmSptOBTw6sCBuFS4OeepzuaVuAqcfXjAmX0hFcu3lvZjwlvwq8FuMvnQQchF0GNE2Z88yAi1+O8kRtV9TOBbEsqgKq2414D/0hE/jGML1yFaCF/9dmfA9OAwgkaKozn4Z2P8/j8CNP4tpCncKOqdgIduFJDQfMqXBWKnXn+HmafCxWrbAfmi6uSMqkx4zsKcXwU+DRwiareVSnZqroPZ4CbgbtE5LRKyQ4CEZkBbATu8/u79wBrJX6vlVcDP/I8Pj8eAZq8taiB4T1gr8aVPcpHWEbQb8Irl7uB10nA9flEZBauIOqP/P7urfG9k5PA+zXjm4OI1APfwL06r1PV7ZXWQV0RvyuBfUC7iKQrrUMZXAE8pKpHCxwTxyVnLeT31sM0CBuB/Z6Hm4/NhLMCoIUCpdK9zR2P4cIiQXI18MMCDzo4SUIPZnw9RGQ2Ls6VBC7UCGuAebGwm3CVXB8WkWpZeF7odXKIh4AzRWRBBfQZF8+zey3+E165hOGB5pvwyuVXQBZYNc5xReO90s/DefSFiKrPPwLO876TkxYzvgxvFf4prhz19aPWW0aCtxLi83hl00XkLVHrVAhvBvs64J5Cx3mTlpVYR1ostwFdRWzj/TGwPKg12Z4n28I4DysvLPA48I9ByPW4DriziG289wBv9raHl42InILb5TjePZLFxd+vCUJuXDnpja+IfBm3q+fjqvoXxW4VrhSqeg/weuCTIvJEjDcpzMGV1M6bxSmHOK16mAE8Od5B3uqNHxCcJ7gGGMB5tuPxMFAXhNBCqxx8eAa3NjsoD/RK4NEi81W0Ar8XkNxYctIn1hGRX+GWvXw4al0KISLn4WaCz1bV30atTzl4ntRvcUlrijE+sUBEvgm8NYgczCLyG2Caqi4sX7MJyV2Jy10xW1UPV1h2H874vq6IY5cATwMLqv1+z8dJb3yNaBCRFtwW5Kq5Ab348GtU9f4A2loNHPbZXRYq3pvTVap6ZyXlerJfBzxZbKY2EblOVYvx0KsSM76GYRgRENf44YRIJpPdIqLl/iSTye6o+5KPqPpYrWNbjXpHqXM1jle1Myk8XzkJaqpF1cdy5KbTaTo6Oko6N5dSKhhEpXd9fT0DA347q4ujnGtc7nhX43gNUY1VLiat8d2yZQvJZJKFCxcyODiIqpLJZMhms5x22mm+Jcirzfj69TGbzXLo0CGamppobm72a6ds49ve3k5tbS1z586lpqYGVaWnp4fGxkaee+451q1blytrTJttbW15y4avWLEikCrP+fRWVV7xilcMj1dHR8dwP4bGy09vP52Hysrn6lxOn/Od66d3d3c32WwWgAsvvHBCevuNtd+57e3tTJ8+nTlz5oz4DmUymWGZYfW5EvdI1EyKsMNo2traOHbsGC+88ALbt28nkUiQzWbJZDLU1NSQSCSiVrFsCvXxpZdeYurUqaHJPuecc+jr6+NXv/oV+/btY//+/aTTaV56ya3SG8+Tqamp4fDhw4i30qy/v58lS5Zw2mmnjflSBUkmk+H48eNs376dffv2kUwmmTt3LplMhjPOKJw7fNOmTUyZMmVY7927d5NIJFi6dClPPfXUuLL9+rx48WKmTZs2bp/99G5oaCCRSLB+/foJyy12rDOZDM8999wIuYlEgkQiUZS3Wk6fo7pHKsmk9Xy3bdtGT08Ps2fPRlVJp9OkUikOHTrEmWeema+d2D49i+1jY2Mje/bsYdUq/w1RQYUd/GQ3NDTQ2dnJ8uXLc2VNpJuB6Byl3mGcW4zOUcqOarx82onldzcfk9b4lthObC9gNcZ8RYQtW7bkLVd+3nnncejQIbq6ugqWSt+4cWNgxrdYvffv3z+mrLyIjFvefenSpQX77Feifej8DRs2lGWI8ul95MiRgnKH9A7rOo/X52LP9SstX849EjXVmrZwDB0dHSMu3t69e6mrq8t74VOpFNOmTaO2tpb9+/dzwQUXRN2FcRndx6Ev1pAR6+vrY2BgIK8RK7WPW7dunbABTaVSLFiwgIsuuqjsficSiWJ2zY1h9Hh1dnbS2Ng4bEAB3/FasGAB6XS6JF3r6+vL6rPfWBdr+MvRG/LfX37foa6urmGDWM51Lne8hij1HomSSeH5JpPJ7mw2W/ae+zjPmEbVx2od22rUO0qdq3G8qp1JMeHW39/fpKrivXasAQ4D5wx9lvuD28u/G7hh9N/ifNPk9nFUfz4B/IP3+yrgv4Eav2NL6WMBuXOAo0DS+38bcHVQcsulgN43Ad/0fl+Ay10wNQ5659PZ0/Uh3M40AT4IfC1InQuM1+/jch0LcArwPJCKw3hVO5PC+A4hLlHzf+JK/uz2O0ZVn8clFrlNRNZWUr+QaOHlJCnbcaGkShTjvBp4QF0GKqie2lsteOOlLmfA08CFhU6IGhGZg6s88WPvo83ANeKqD4fNcBIeVX0B+AkuQY5RJpPG+Ho34reAb6vq9wodq6q/AW4EvitVnDNUXGWFucCjMKLeWEsFxLcwMjNWK3B1hQxCSYhII7AeyM3NUA0PjauBHww96NRVPDkIhJrnWUTqcIY2t9JGHJPhVyWTxvgCfwMIrhLwuKhL2PFN4NtSvfXS/PKythKyMZGX87LeO/SZugQxvwXiPHN5JbDF8+CGaMVViojzd8EvAXkr4T80LgSe1pFZxe4GLhFXddkogzjfcEUjIr8HvAV4i/pXRM3HrcCLuITa1UgLY7+U7cBCccnNw+INwCOqmhn1edy9ohZG5bFV1V/jYtero1BoPETkVGATOQ86j7DKC+XSwtjxOoxLSXlxiHJPCqre+IrIUuBLwBtV9XcTOdfzGN8CvElE3hSGfmEhrqLCcl6OAwLDJYjuIlwjmK8UTCvxSZI+As9TuwT/ckGtxDf0cBnwsLrafrnswL3pLR97Svl4bwItFLjOYcg9mahq4+t5BZuBP1fVx0tpQ1Wfxc3o/rO4ckLVwtXA/eoqLIwmtDimiEzFFcr0q7i7E1eO/LwwZJfJJbhcsn4P6DjHfX2rTnjx/VbC03s18Lz3ZjCaVlx8v1rDdbGgao2v5139K7BVVb9eTluq+gTwZ8BmqZ5y7YUKEf4IWBnSZOKFwG9UtWv0Hyo84TdRWshfOudx4BTvLSo2eA+6y8lfWj7MsW4hz3ipagfQSbzj+7Gnao0v8CFgPnBzEI2p6u24V/jbYz75gojMAF7D2DggAOrKcv8QuCoE8eNVn20lZl6k56FdTR691dXtayVmeuOqKu9W1Xw5ch8G5ourRhw0VXedq41YG5l8iMjFOKP7xjyv3aVyC27p1p8H2GYYXAa0q+rRAscE/iqdEwcsVNrlEaDJWwYXFzYCz3geWz5aiZ8xKVjo0puzuJOAvV/vDeBU3BtBPuJUBLUqqTrj683i/xuukOGBINtW1ePAG4H3icilQbYdMMVUn70XuMhbFhYUa4CMqubNoRiWQSiTFsYfr63AmSKyIHx1xsd70F1LYe8T7+8tAYtvwdXXK1TJ+5fACWBFwLJPGqrK+IpIAvge8ClV3RKGDG9N4x8A3xCRdBgyykFEmoBrcAYuL+qKFD6K85KDotiS47GZwPI8s3H1VtUTwD04gxcH1gHPqurT4xz3I+C8gOP7xYzXUHw/Fte5Gqkq4wv0AMeBz4QpRFXbgC8C+8RVrI0TrwGm4cZiPIKekGmhOOP7Y2C5ty02albgPLRfFnFsK/Hx2FsoYqy9XW8P4GLaZSMi84EluDeB8Yjr5GpVUG3G91+Ad3tP3bD5O+AbOGMfG1T1u14Sk2LG4BHgbUEYQRF5J3A28PMidBwABoE7ypUbAN8B+oscr63AxeLKukeGt0X7Q7ile8XQA3wtIPHfBE54bwLj8SjuIXt9QLJPKqpqnZ6q3lJBWVngnZWSFxI7gE8DzwbQ1lbgwxN48L0biEO+0tuA3xRzoKo+KyJ/W+zxYaGqL4rIpxg/3jvEbcChgMR/jiIdDlV9SUT+ArfqwpggkyKfr2EYRrURWdghmUx2i4iW+5NMJvOtgawqveI6HoZhhENknq/EtO5aVHqVIzedTtPRUWgJa2Hq6+uLqkYb9Lm5VLLCRlQ6Q3R6l9vnIMbMqlyMJFbGd8uWLUydOpV0Os3g4CCqSiaTIZvNsm7dunzthG5829vbGRwcpLm5eVivbDbLoUOHmD17NkuXjt2VGoTxbW9vp7a2lrlz51JTU4Oq0t3dPfz72rVrc2WNabOtrW3E58uWLaOrq4uBgQFWrFgxXILb7/xKnDuq/4E8rIqRXe54lapzPr2juk6Vkp3T96orchkmsTG+bW1tHD16lGnTppHNZlm9ejW9vb1kMhmmTZvGjBkzWLRobJbEShjfe+65h2QySTabZfr06SxbtmxYtzCNSW9vL9u3b6e/v5/p092KNz/Z+b5YE5BddaW/y3lTiEpn75xI9I7yHhnVhhlfj9gY3zLaOenDDvm+GNu2baOnp4fZs2ejqqTTaWbOnMmBAwdGeOt+51fiXJ8+lD1excgud7xK1TlKvaO8R8oZr8lMrJaa+V3MhoYG9uzZw/nnnx87vTo7O1m+PJR0qhOW7VdyfObMmdTW1g6X/u7q6qK3t5fFixfT3t4+XHIcxpYNnzJlCvPnz897biqVoqampuC5uWXl/eSGUdJ+tN59fX1jZBc699xzzx0u0T6e3qXiV6K9traW1atXD5do7+3tpaGhgWeffZZHHnkk73Xq7Oxk1qxZLFy4cNxzixmv3D7v3r17uDy8n+xi769yx2uyEqnnu3///hEXc+/evdTV1Q1fzEwmQzKZZPHixezatWv4Yp5++uns2LGDCy64IBTPd7RenZ2diEhevYYMUTl6iYhu2bJlzJdjSPaQIevr62NgYGDEmFx//fVl3dw24VYaNuE2MWzCbSSRGd9ybsJcgr6gUekV1/EwDCMcIlvn29/f3+RtkxXPQ7wAV5E1NerzWuCnwA25nw/9BG1oRuuVo8dfAZ/xfr8Cl9JxzHGl6lVA7mxcjbFpuOv1NLA6KLmGYURDLHI7iMvY/xXgFnXZuIZRl6LwPcBt4uqWRUVupqcHgVdWSJ+rgR+q6lB+AsskZRiTgFgYX1wJnw7gu35/VNUdwNeBz1ZQp2HEVQo4HW8Pu7rEMfdTmfSDoysKmPE1jElA5MZXRJqBPwVuGmet1SeAtSISZH7aYmkB7vS88CFCT6cnLhH6Rbg8s0M8BjSKyFlhyjYMI1wiNb4iIriy759U1f2FjlXVPuC9wBel8jl2/ZJL3wdsFFdPLSzeADyaG4pRV13g+1geVcOoaqL2fN8ONOLS2I2Lqj4AtAMfC1OpXMRVCHgVLkF4ri5HgW24ybewaME/raCFHgyjyonM+IrILOBTuFUMgxM49U+Bd4rIilAUG8s1wAPq8vuOJrTQg4jUAVfiXzZ8C3C2iMwLQ7ZhGOETpef7aeBbqjpuZYRcVPUQ8L+Br4rL+B82hepZ3QlcJiJjkzuUz0XAU+pqyo1AXaHP+4hPvTHDMCZIJMZXRF4PvBa4tcQm/gV4AbgpMKV8EJFTgU24SsBjUNUeYBfwuhDEt1C4kkErFnowjKql4sZXRJK4Sbb3qeqxUtrwVkXcCNwqIguD1G8UlwEPq+qRAscEHn8VVza8hcIFFO8DXi0iDUHKNgyjMkTh+X4E2K6qd5XTiKr+BvgC8I/eqokwKKZUeitwTcAhkNXAEa+PvngPrq2EO+FnGEZIVNT4isgrcR7rzQE1+XfAWYTw+u3turscF9fNi6r+N6544foAxY/eWJGPViz0YBhVSaU9353A11S1K4jGvJ1mHwS+JyJjM62Xx2uB3ap6sIhjgw49FONxg3swXCoiiQBlG4ZRASptfD+PK3MdJPd77T4XcLu347Y8F0M78EEvnl0WIvJOYAkw7ioQVf0d8CLwD+XKNQyjslQ0mbqqBhVuyG3zJYILY+RyHy6fRDE8hNuEcTwAuR3A7V6/iuHzwDMByDUMo4JEls/XMAzjZKbksEMymewWES33J5lMdleTbMMwjCAo2fONsgBmXItvGoZhFEvgMd8tW7Zw6qmnMm/ePAYHB1FVMpkMmUyGRCLBunXrghYJQHt7O9OnT2fOnDnDcru7u6mpqUFVWbt2LQDpdJqOjpfn0USkaCseZR2sKOuOGYYRPIEa37a2No4dO8ZLL71ET08Pq1evpquri4GBAWpqamhsbAxS3AgymQwDAwN0dXUxffp0li1bRkNDA5lMhhUrVgwf19HRMaYEdltb24jPli1bNqz3ihUrqK93qRvyld8u5/xKnJtLxNVADMPwCNT4btq0acxnc+bMCVJEXq688sqSZW/atGlEifbdu3eTTqeZOXMm+/btY+nSpQXPr6mpGVHevb+/nyVLlnDgwAFfAxiHcw3DiJbAY765RkxVSafTNDY2smfPHlatWuXXTmAxXz/ZDQ0NdHZ2snz58lx5ExE3WteSzo1a9qh2LM5tGBFTlue7detW0uk0Bw4coLm5mRMnTjBlyhTmz5/PvHnz6Ovro6uri97eXs4880za29upqakhlUqRSCQ4cOBAybI7OjpGyD169Ci1tbWsXr2avr4+MpkMvb29NDQ0cPToUR555BFSqRRz5syh1FQQ9fX1JZ8LUFdXV/L55ZybSyKR6Cm7EcMwyqZkzzeZTHZns9my44elTAAFKTubzS4bXTG5ECLSmHv86P9P5PyozjUMI3pC2WQhIvcC/6qq3xGRL+OSgoe+BVZELgX+SlU3iMgy4AHgjEDWpRmGYQRI4LkdvIKSG3E5F6Cymbdys4H9GugDzq+QbMMwjKIJI7HOFcBDXoFJgAeBV4a9xMlLQH4tXjYwz9u1QpOGYcSSMIxvCzm5aL20j/fjClGGyVqgV1WfzvmsFSuxbhhGDAnU+HqFJC9jbALySnigfjlwHwMaReSskGUbhmFMiKA939cBO73CkrncB2z04sGB45URGmN8vbSMrVjowTCMmBG08fUtf+PFf7fhyvKEwTIgATzh87dWLPRgGEbMCMz4egUkryV/7bEwQw/XAa15lpRtAc4WkdNDkm0YhjFhgvR81wM9XkFJP+4ELvPiwkHTQh6jr6rHcWGPsCf8DMMwiiZI41uw6KMXB96FiwsHhogsBBYDbQUOsyVnhmHEikCMrzfh1cL4FXc3E3z89VrgblUdLHDM/cCrRaQhYNmGYRglEZTn+0pckp5fjHNcK3CtFx8OCt9JvlxU9RiwFbcBxDAMI3KCMr7XAZvHy6HgxYMP4eLDZSMiM4HVuBwO42GhB8MwYkPZxldE6oAP4bYRF8MTwBfLlevxeWCfqvYVcey9wHUicmZAsg3DMEomCM9XgKPAo0Ue/yDwQgByAQYpPNGWSw/QAZwWkGzDMIySCSWlpGEYhlGYMBLrGIZhGONgxtcwDCMCCtZwK6dcT319PQMDAyUpFdW5uZRS3sgwDKNYCsZ8/aoEt7W1jaiiu2zZMnp7e8lkMqxYsWK4ZLlftV2/c7u6uhgYGBj33GLPL+fcUX23Kr+GYYTGhMMOmzZtYsqUKRw+fBgRYffu3SQSCZYuXcpTTz014XMbGxs599xz2bdv3/jK1tQMnwvQ39/PkiVLOO2008YYzyDPNQzDCJoJe75FN5zHA43zuT7tmOdrGEYoFIz51tXVPSciqVIarqurG/YyJ8pQ+KDS5+aSSCRGJ4Q3DMMIjAmt8xWRRlXtLeX4cs6NWrZhGEbQ2CYLwzCMCLB1voZhGBFgxtcwDCMCzPgahmFEgBlfwzCMCDDjaxiGEQFmfA3DMCLAjK9hGEYEmPE1DMOIADO+hmEYEWDG1zAMIwLM+BqGYUSAGV/DMIwIMONrGIYRAWZ8DcMwIuD/AcSKa68QnEqoAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Include decision tree visualization here\n",
        "tree.plot_tree(clf)\n",
        "# Discuss what the model has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfAQCjEbCmL4"
      },
      "source": [
        "## 6. (optional 5% extra credit) Implement reduced error pruning to help avoid overfitting.  \n",
        "- You will need to take a validation set out of your training data to do this, while still having a test set to test your final accuracy. \n",
        "- Create a table comparing your decision tree implementation's results on the cars and voting data sets with and without reduced error pruning. \n",
        "- This table should compare:\n",
        "    - a) The # of nodes (including leaf nodes) and tree depth of the final decision trees \n",
        "    - b) The generalization (test set) accuracy. (For the unpruned 10-fold CV models, just use their average values in the table)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSdw7W9fCmL4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab_3_decision_tree.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "bf6171e0b005234b90bf150e7dae75bb4dd3ef3eda0e7c742cc9ee610eeebd82"
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit ('.venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
