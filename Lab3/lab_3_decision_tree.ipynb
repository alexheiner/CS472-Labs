{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mADd6iSDCmLs"
      },
      "source": [
        "# Decision Tree Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "CGsI9XEJCmLv"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arff Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import re\n",
        "import warnings\n",
        "import sys\n",
        "import logging\n",
        "import copy\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(\"INFO\")\n",
        "logging.basicConfig()\n",
        "\n",
        "\n",
        "class Arff:\n",
        "    \"\"\"\n",
        "    Contains arff file data.\n",
        "    For discrete attributes, at least one value must be a float in\n",
        "    order for numpy array functions to work properly. (The load_arff\n",
        "    function ensures that all values are read as floats.)\n",
        "    To do: Change backend to use Pandas dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, arff=None, row_idx=None, col_idx=None, label_count=None, name=\"Untitled\", numeric=True, missing=float(\"NaN\")):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            arff (str or Arff object): Path to arff file or another arff file\n",
        "            row_start (int):\n",
        "            col_start (int):\n",
        "            row_count (int):\n",
        "            col_count (int):\n",
        "            label_count (int):\n",
        "        \"\"\"\n",
        "\n",
        "        self.data = None\n",
        "        self.dataset_name = name\n",
        "        self.attr_names = []\n",
        "        self.attr_types = []\n",
        "        self.str_to_enum = []  # list of dictionaries\n",
        "        self.enum_to_str = []  # list of dictionaries\n",
        "        self.label_columns = []\n",
        "        self.MISSING = missing\n",
        "        self.label_count = label_count\n",
        "        self.numeric = numeric\n",
        "\n",
        "        if isinstance(arff, Arff): # Make a copy of arff file\n",
        "            logger.debug(\"Creating ARFF from ARFF object\")\n",
        "            if self.dataset_name == \"Untitled\":\n",
        "                name = arff.dataset_name+\"_subset\"\n",
        "            self._copy_and_slice_arff(arff,row_idx, col_idx, label_count, name)\n",
        "        elif isinstance(arff, str) or (sys.version_info < (3, 0) and isinstance(arff, unicode)):  # load from path\n",
        "            logger.debug(\"Creating ARFF from file path\")\n",
        "            self.load_arff(arff)\n",
        "            if label_count is None: # if label count is not specified, assume 1\n",
        "                label_count = 1\n",
        "                warnings.warn(\"Label count not specified, using 1\")\n",
        "            self._copy_and_slice_arff(self, row_idx, col_idx, label_count, name)\n",
        "        elif isinstance(arff, np.ndarray): # convert 2D numpy array to arff\n",
        "            logger.debug(\"Creating ARFF from ND_ARRAY\")\n",
        "            self.data = arff\n",
        "            if label_count is None:\n",
        "                warnings.warn(\"Label count not specified, using None\")\n",
        "            self._copy_and_slice_arff(self, row_idx, col_idx, label_count, name)\n",
        "        else:\n",
        "            logger.debug(\"Creating Empty Arff object\")\n",
        "            # Empty arff data structure\n",
        "            pass\n",
        "\n",
        "        # Initialize vacuous if data defined\n",
        "        if not self.data is None:\n",
        "            columns = self.data.shape[1]\n",
        "            self.attr_names = [x for x in range(columns)]         if not self.attr_names else self.attr_names\n",
        "            self.attr_types = [\"Unknown\" for x in range(columns)] if not self.attr_types else self.attr_types\n",
        "            self.str_to_enum = [{} for x in range(columns)]       if not self.str_to_enum else self.str_to_enum\n",
        "            self.enum_to_str = [{} for x in range(columns)]       if not self.enum_to_str else self.enum_to_str\n",
        "            self.label_columns = []\n",
        "\n",
        "    def set_size(self, rows, cols):\n",
        "        \"\"\"Resize this matrix (and set all attributes to be continuous)\"\"\"\n",
        "        self.data = np.zeros((rows, cols))\n",
        "        self.attr_names = [\"\"] * cols\n",
        "        self.str_to_enum = []\n",
        "        self.enum_to_str = []\n",
        "\n",
        "    def load_arff(self, filename):\n",
        "        \"\"\"Load matrix from an ARFF file\"\"\"\n",
        "        self.data = None\n",
        "        self.attr_names = []\n",
        "        self.str_to_enum = []\n",
        "        self.enum_to_str = []\n",
        "        reading_data = False\n",
        "\n",
        "        rows = []  # we read data into array of rows, then convert into array of columns\n",
        "\n",
        "        with open(filename) as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.strip()  # why was this rstrip()?\n",
        "                if len(line) > 0 and line[0] != '%':\n",
        "                    if not reading_data:\n",
        "                        if line.lower().startswith(\"@relation\"):\n",
        "                            self.dataset_name = line[9:].strip()\n",
        "                        elif line.lower().startswith(\"@attribute\"):\n",
        "                            attr_def = line[10:].strip()\n",
        "                            if attr_def[0] == \"'\":\n",
        "                                attr_def = attr_def[1:]\n",
        "                                attr_name = attr_def[:attr_def.index(\"'\")]\n",
        "                                attr_def = attr_def[attr_def.index(\"'\") + 1:].strip()\n",
        "                            else:\n",
        "                                search = re.search(r'(\\w*)\\s*(.*)', attr_def)\n",
        "                                attr_name = search.group(1)\n",
        "                                attr_def = search.group(2)\n",
        "                                # Remove white space from atribute values\n",
        "                                attr_def = \"\".join(attr_def.split())\n",
        "\n",
        "                            self.attr_names += [attr_name]\n",
        "\n",
        "                            str_to_enum = {}\n",
        "                            enum_to_str = {}\n",
        "                            if attr_def.lower() in [\"real\", \"continuous\"]:\n",
        "                                self.attr_types.append(\"continuous\")\n",
        "                            elif attr_def.lower() == \"integer\":\n",
        "                                self.attr_types.append(\"ordinal\")\n",
        "                            else:\n",
        "                                # attribute is discrete\n",
        "                                assert attr_def[0] == '{' and attr_def[-1] == '}'\n",
        "                                attr_def = attr_def[1:-1]\n",
        "                                attr_vals = attr_def.split(\",\")\n",
        "                                val_idx = 0\n",
        "                                for val in attr_vals:\n",
        "                                    val = val.strip()\n",
        "                                    enum_to_str[val_idx] = val\n",
        "                                    str_to_enum[val] = val_idx\n",
        "                                    val_idx += 1\n",
        "                                self.attr_types.append(\"nominal\")\n",
        "                            self.enum_to_str.append(enum_to_str)\n",
        "                            self.str_to_enum.append(str_to_enum)\n",
        "\n",
        "                        elif line.lower().startswith(\"@data\"):\n",
        "                            reading_data = True\n",
        "\n",
        "                    else:\n",
        "                        # reading data\n",
        "                        val_idx = 0\n",
        "                        # print(\"{}\".format(line))\n",
        "                        vals = line.split(\",\")\n",
        "                        if self.numeric:\n",
        "                            row = np.zeros(len(vals))\n",
        "                        else:\n",
        "                            row = np.empty(len(vals), dtype=object)\n",
        "\n",
        "                        for i,val in enumerate(vals):\n",
        "                            val = val.strip()\n",
        "                            if not val:\n",
        "                                raise Exception(\"Missing data element in row with data '{}'\".format(line))\n",
        "                            else:\n",
        "                                if self.numeric: # record indices for nominal variables\n",
        "                                    row[val_idx] = float(\n",
        "                                        self.MISSING if val == \"?\" else self.str_to_enum[val_idx].get(val, val))\n",
        "                                else: # record actual values\n",
        "                                    row[val_idx] = self.MISSING if val == \"?\" else val\n",
        "\n",
        "\n",
        "                                # Capture missings in str_to_enum\n",
        "                                # if val == \"?\" and self.str_to_enum[i] and not \"?\" in self.str_to_enum:\n",
        "                                #\n",
        "                                #     num = max(self.str_to_enum[i].values())\n",
        "                                #     self.str_to_enum[i][\"?\"] = num\n",
        "                                #     self.enum_to_str[i][num] = \"?\"\n",
        "\n",
        "                            val_idx += 1\n",
        "                        rows += [row]\n",
        "        self.data = np.array(rows)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def instance_count(self):\n",
        "        \"\"\"Get the number of rows in the matrix\"\"\"\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    @property\n",
        "    def features_count(self):\n",
        "        \"\"\"Get the number of columns (or attributes) in the matrix\"\"\"\n",
        "        return self.data.shape[1] - self.label_count\n",
        "\n",
        "    def create_subset_arff(self, row_idx=None, col_idx=None, label_count=None):\n",
        "        \"\"\" This returns a new arff file with specified slices; both objects reference same underlying data\n",
        "        Args:\n",
        "            row_idx (slice() or list): A slice or list of row indices\n",
        "            col_idx (slice() or list):  A slice or list of col indices\n",
        "            label_count (int): The number of columns to be considered as \"labels\"\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        new_arff = Arff(arff=self, row_idx=row_idx, col_idx=col_idx, label_count=label_count) # create a copy\n",
        "        return new_arff\n",
        "\n",
        "    def _copy_and_slice_arff(self, arff=None, row_idx=None, col_idx=None, label_count=None, dataset_name=\"Untitled\"):\n",
        "        \"\"\" This copies an external arff to the current arff object, slicing as specified\n",
        "        Args:\n",
        "            row_idx (slice() or list): A slice or list of row indices\n",
        "            col_idx (slice() or list):  A slice or list of col indices\n",
        "            label_count (int): The number of columns to be considered as \"labels\"\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        def slicer(_list, idx):\n",
        "            \"\"\" If a list is specified as a slice, loop through. Idx should be a list, int, or slice.\n",
        "                Returns:\n",
        "                    A list!!\n",
        "            \"\"\"\n",
        "            try:\n",
        "                if isinstance(col_idx, list):\n",
        "                    return [_list[i] for i in col_idx]\n",
        "                elif isinstance(col_idx, int):\n",
        "                    return [_list[idx]]\n",
        "                elif isinstance(col_idx, slice):\n",
        "                    return _list[idx]\n",
        "                else:\n",
        "                    raise Exception(\"Unexpected index type\")\n",
        "            except:\n",
        "                warnings.warn(\"Could not slice {} element of Arff object, returning None\".format(_list))\n",
        "                return None\n",
        "\n",
        "        if self.is_iterable(row_idx) and self.is_iterable(col_idx):\n",
        "            warnings.warn(\"User is attempting to slice both axes using lists. This will result in a 1D array, \" \\\n",
        "                          \"is not supported by the toolkit, and may not be what the user intended.\")\n",
        "\n",
        "        # Fix row indices\n",
        "        if row_idx is None:\n",
        "            row_idx=slice(0,None)\n",
        "        elif isinstance(row_idx, int):\n",
        "            row_idx = slice(row_idx,row_idx+1) # make it a list, to preserve dimension\n",
        "        if col_idx is None:\n",
        "            col_idx=slice(0,None)\n",
        "        elif isinstance(col_idx, int):\n",
        "            col_idx = slice(col_idx,col_idx+1)\n",
        "\n",
        "        # If reference has label count, but current one doesn't, infer it\n",
        "        column_count = arff.shape[1]\n",
        "        if label_count is None and arff.label_count:\n",
        "            label_list = [1 if i in range(column_count-arff.label_count, column_count) else 0 for i in range(column_count) ]\n",
        "            self.label_count = sum(slicer(label_list, col_idx))\n",
        "        else:\n",
        "            self.label_count = label_count\n",
        "\n",
        "        ## Update main numpy array\n",
        "        self.data = arff.data[row_idx, col_idx]\n",
        "        if len(self.shape) < 2:\n",
        "            warnings.warn(\"Unexpected array dimension (should be 2, not {})\".format(len(self.shape)))\n",
        "\n",
        "        ## Update all other features\n",
        "        self.dataset_name = dataset_name\n",
        "        self.attr_names = slicer(arff.attr_names,col_idx)\n",
        "        self.attr_types = slicer(arff.attr_types,col_idx)\n",
        "        self.str_to_enum = slicer(arff.str_to_enum,col_idx)\n",
        "        self.enum_to_str = slicer(arff.enum_to_str,col_idx)\n",
        "\n",
        "    def get_features(self, row_idx=None):\n",
        "        \"\"\" Return features as 2D array\n",
        "        Args:\n",
        "            _type: Optionally specify 'nominal' or 'continuous' to return appropriate subset of features\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        if row_idx is None:\n",
        "            row_idx = slice(0,None)\n",
        "        end_idx = None if self.label_count == 0 else -self.label_count # return all if no labels\n",
        "        return self.create_subset_arff(row_idx=row_idx, col_idx=slice(0,end_idx), label_count=0)\n",
        "\n",
        "    def get_labels(self, row_idx=None):\n",
        "        if row_idx is None:\n",
        "            row_idx = slice(0,None)\n",
        "\n",
        "        start_idx = self.shape[1] if -self.label_count == 0 else -self.label_count # return nothing if no labels\n",
        "        new_arff = self.create_subset_arff(row_idx=row_idx, col_idx=slice(start_idx, None), label_count=self.label_count)\n",
        "        return new_arff\n",
        "\n",
        "    def attr_name(self, col):\n",
        "        \"\"\"Get the name of the specified attribute\"\"\"\n",
        "        return self.attr_names[col]\n",
        "\n",
        "    def set_attr_name(self, col, name):\n",
        "        \"\"\"Set the name of the specified attribute\"\"\"\n",
        "        self.attr_names[col] = name\n",
        "\n",
        "    def get_attr_names(self):\n",
        "        return self.attr_names\n",
        "\n",
        "    def attr_value(self, attr, val):\n",
        "        \"\"\"\n",
        "        Get the name of the specified value (attr is a column index)\n",
        "        :param attr: index of the column\n",
        "        :param val: index of the value in the column attribute list\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self.enum_to_str[attr][val]\n",
        "\n",
        "    def unique_value_count(self, col=0):\n",
        "        \"\"\"\n",
        "        Get the number of values associated with the specified attribute (or columnn)\n",
        "        0=continuous, 2=binary, 3=trinary, etc.\n",
        "        \"\"\"\n",
        "        values = len(self.enum_to_str[col]) if self.enum_to_str else 0\n",
        "        return values\n",
        "\n",
        "    def is_nominal(self, col=0):\n",
        "        nominal =self.unique_value_count(col) > 0\n",
        "        return nominal\n",
        "\n",
        "    def get_arff_as_string(self):\n",
        "        \"\"\" Print arff class as arff-style string\n",
        "            Returns:\n",
        "                string\n",
        "        \"\"\"\n",
        "        out_string = \"\"\n",
        "        out_string += \"@RELATION {}\".format(self.dataset_name) + \"\\n\"\n",
        "        for i in range(len(self.attr_names)):\n",
        "            out_string += \"@ATTRIBUTE {}\".format(self.attr_names[i])\n",
        "            if self.is_nominal(i):\n",
        "                out_string += (\" {{{}}}\".format(\", \".join(self.enum_to_str[i].values()))) + \"\\n\"\n",
        "            else:\n",
        "                out_string += (\" CONTINUOUS\") + \"\\n\"\n",
        "\n",
        "        out_string += (\"@DATA\") + \"\\n\"\n",
        "\n",
        "        ## i idx\n",
        "        for i in range(self.shape[0]):\n",
        "            r = self.data[i]\n",
        "            values = []\n",
        "\n",
        "            # j idx\n",
        "            for j in range(len(r)):\n",
        "                if not self.is_nominal(j):\n",
        "                    if not self.is_missing(r[j]):\n",
        "                        values.append(str(r[j]))\n",
        "                    else:\n",
        "                        values.append(\"?\")\n",
        "                else:\n",
        "                    try:\n",
        "                        if self.numeric:\n",
        "                            values.append(self.enum_to_str[j][r[j]])\n",
        "                        else:\n",
        "                            values.append(r[j])\n",
        "                    except(Exception) as e:\n",
        "                        #print(out_string,values)\n",
        "                        if self.is_missing(r[j]):\n",
        "                            values.append(\"?\")\n",
        "                        else:\n",
        "                            raise e\n",
        "\n",
        "            # values = list(map(lambda j: str(r[j]) if self.value_count(j) == 0 else self.enum_to_str[j][r[j]],\n",
        "            #                   range(len(r))))\n",
        "            out_string += (\"{}\".format(\", \".join(values))) + \"\\n\"\n",
        "\n",
        "        return out_string\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.get_arff_as_string()\n",
        "\n",
        "    def print(self):\n",
        "        print(self)\n",
        "\n",
        "    def nd_array(self, obj):\n",
        "        \"\"\" Convert an arff, list, or numpy array to numpy array\n",
        "        Args:\n",
        "            obj (array-like): An object to be converted\n",
        "        Returns\n",
        "            numpy array\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(obj, Arff):\n",
        "            return obj.data\n",
        "        elif isinstance(obj, list):\n",
        "            return np.ndarray(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj\n",
        "        else:\n",
        "            raise Exception(\"Unrecognized data type\")\n",
        "\n",
        "    def get_nominal_idx(self):\n",
        "        nominal_idx = [i for i,feature_type in enumerate(self.attr_types) if feature_type==\"nominal\"]\n",
        "        return nominal_idx if nominal_idx else None\n",
        "\n",
        "    def reshape(self, tup):\n",
        "        if self.is_iterable(tup):\n",
        "            return self.data.reshape(*tup)\n",
        "        return self.data.reshape(tup)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Trivial wrapper for the 2D Numpy array data\n",
        "        Args:\n",
        "            index: Index, slice, etc. whatever you would use for Numpy array\n",
        "        Returns:\n",
        "            array-like object\n",
        "        \"\"\"\n",
        "\n",
        "        ## This will slice ARFF and return smaller arffs; it's considerably slower than numpy slicing\n",
        "        # if not self.is_iterable(index):\n",
        "        #     index = [index, slice(0,None)]\n",
        "        # x = self.create_subset_arff(index[0], index[1])\n",
        "        # return x\n",
        "        return self.data[index]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self.data[key] = value\n",
        "\n",
        "    def copy(self):\n",
        "        return copy.deepcopy(self)\n",
        "\n",
        "    def is_iterable(self, obj):\n",
        "        try:\n",
        "            iter(obj)\n",
        "        except TypeError as te:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Trivial wrapper for looping Numpy 2D array\n",
        "        \"\"\"\n",
        "        for i in self.data:\n",
        "            yield i\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        return self.data.T\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        import pandas as pd\n",
        "        df = pd.DataFrame(data=self.data, columns=self.attr_names)\n",
        "        return df\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self.data.shape\n",
        "    # __iter__() and __getitem__()\n",
        "\n",
        "    def is_missing(self, value):\n",
        "        if self.MISSING in [np.inf, \"?\"]:\n",
        "            return value == self.MISSING\n",
        "        elif np.isnan(self.MISSING):\n",
        "            return np.isnan(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgq5w2mOCmLw"
      },
      "source": [
        "## 1. (40%) Correctly implement the ID3 decision tree algorithm, including the ability to handle unknown attributes (You do not need to handle real valued attributes).  \n",
        "### Code Requirements/Notes:\n",
        "- Use standard information gain as your basic attribute evaluation metric.  (Note that normal ID3 would usually augment information gain with gain ratio or some other mechanism to penalize statistically insignificant attribute splits. Otherwise, even with approaches like pruning below, the SSE type of overfit could still hurt us.) \n",
        "- You are welcome to create other classes and/or functions in addition to the ones provided below. (e.g. If you build out a tree structure, you might create a node class).\n",
        "- It is a good idea to use a simple data set (like the lenses data or the pizza homework), which you can check by hand, to test your algorithm to make sure that it is working correctly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "XtB9lCVaCmLw"
      },
      "outputs": [],
      "source": [
        "class DTClassifier(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self,counts=None, target_name=''):\n",
        "        \"\"\" Initialize class with chosen hyperparameters.\n",
        "        Args:\n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            counts: A list of Ints that tell you how many types of each feature there are\n",
        "        Example:\n",
        "            DT  = DTClassifier()\n",
        "            or\n",
        "            DT = DTClassifier(count = [2,3,2,2])\n",
        "            Dataset = \n",
        "            [[0,1,0,0],\n",
        "            [1,2,1,1],\n",
        "            [0,1,1,0],\n",
        "            [1,2,0,1],\n",
        "            [0,0,1,1]]\n",
        "\n",
        "        \"\"\"\n",
        "        self.counts = counts\n",
        "        print('counts', counts)\n",
        "        self.target_name = target_name\n",
        "\n",
        "\n",
        "    def fit_and_score(self, fullData):\n",
        "        train = np.array(fullData.data[:,0:-1]).astype(int)\n",
        "        targets = np.array(fullData.data[:,-1].reshape(-1,1)).astype(int)\n",
        "        self.fit(train, targets)\n",
        "        \n",
        "        return\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit the data; Make the Decision tree\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 1D numpy array with the training targets\n",
        "\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "\n",
        "        \"\"\"\n",
        "        root = Node()\n",
        "        train_cols = np.transpose(X).astype(int)\n",
        "        nodes = []\n",
        "        for i in range(len(train_cols)):\n",
        "            col = train_cols[i]\n",
        "            temp_split = [[0, [0 for y in range(self.counts[-1]) ]]  for x in range(self.counts[i])]\n",
        "            for j in range(len(col)):\n",
        "                temp_split[col[j]][0] += 1\n",
        "                temp_split[col[j]][1][y[j][0]] += 1\n",
        "            nodes.append(temp_split)\n",
        "        \n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict all classes for a dataset X\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\" Return accuracy(Classification Acc) of model on a given dataset. Must implement own score function.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 1D numpy array of the targets \n",
        "        \"\"\"\n",
        "        return 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Node:\n",
        "  def __init__(self):\n",
        "    self.children=[]\n",
        "    self.indexes_used=[]\n",
        "    \n",
        "\n",
        "  def set_children(self, children):\n",
        "    self.children = children\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybGas3ACmLx"
      },
      "source": [
        "## 1.1 Debug\n",
        "\n",
        "Debug your model by training on the lenses dataset: [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses.arff)\n",
        "\n",
        "Test your model on the lenses test set: [Debug Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses_test.arff)\n",
        "\n",
        "Parameters:\n",
        "(optional) counts = [3,2,2,2] (You should compute this when you read in the data, before fitting)\n",
        "\n",
        "---\n",
        "\n",
        "Expected Results: Accuracy = [0.33]\n",
        "\n",
        "Predictions should match this file: [Lenses Predictions](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv)\n",
        "\n",
        "*NOTE: The [Lenses Prediction](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv) uses the following encoding: soft=2, hard=0, none=1. If your encoding is different, then your output will be different, but not necessarily incorrect.*\n",
        "\n",
        "Split Information Gains (These do not need to be in this exact order):\n",
        "\n",
        "[0.5487949406953987, 0.7704260414863775, 0.3166890883150208, 1.0, 0.4591479170272447, 0.9182958340544894]\n",
        "\n",
        "<!-- You should be able to get about 68% (61%-82%) predictive accuracy on the lenses data -->\n",
        "\n",
        "Here's what your decision tree splits should look like, and the corresponding child node predictions:\n",
        "\n",
        "Decision Tree:\n",
        "<pre>\n",
        "tear_prod_rate = normal:\n",
        "    astigmatism = no:\n",
        "        age = pre_presbyopic:\n",
        "            prediction: soft\n",
        "        age = presbyopic:\n",
        "            spectacle_prescrip = hypermetrope:\n",
        "                prediction: soft\n",
        "            spectacle_prescrip = myope:\n",
        "                prediction: none\n",
        "        age = young:\n",
        "            prediction: soft\n",
        "    astigmatism = yes:\n",
        "        spectacle_prescrip = hypermetrope:\n",
        "            age = pre_presbyopic:\n",
        "                prediction: none\n",
        "            age = presbyopic:\n",
        "                prediction: none\n",
        "            age = young:\n",
        "                prediction: hard\n",
        "        spectacle_prescrip = myope:\n",
        "            prediction: hard\n",
        "tear_prod_rate = reduced:\n",
        "    prediction: none\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "counts [3, 2, 2, 2, 3]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Alex\\AppData\\Local\\Temp/ipykernel_21120/3406363677.py:56: UserWarning: Label count not specified, using 1\n",
            "  warnings.warn(\"Label count not specified, using 1\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data [2 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2]\n",
            "temp_split [[0, [0, 0, 0]], [0, [0, 0, 0]], [1, [1, 0, 0]]]\n",
            "temp_split [[0, [0, 0, 0]], [0, [0, 0, 0]], [2, [1, 1, 0]]]\n",
            "temp_split [[1, [1, 0, 0]], [0, [0, 0, 0]], [2, [1, 1, 0]]]\n",
            "temp_split [[1, [1, 0, 0]], [1, [0, 1, 0]], [2, [1, 1, 0]]]\n",
            "temp_split [[1, [1, 0, 0]], [1, [0, 1, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[2, [2, 0, 0]], [1, [0, 1, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[2, [2, 0, 0]], [2, [1, 1, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[2, [2, 0, 0]], [2, [1, 1, 0]], [4, [2, 2, 0]]]\n",
            "temp_split [[3, [3, 0, 0]], [2, [1, 1, 0]], [4, [2, 2, 0]]]\n",
            "temp_split [[3, [3, 0, 0]], [3, [2, 1, 0]], [4, [2, 2, 0]]]\n",
            "temp_split [[3, [3, 0, 0]], [3, [2, 1, 0]], [5, [3, 2, 0]]]\n",
            "temp_split [[4, [4, 0, 0]], [3, [2, 1, 0]], [5, [3, 2, 0]]]\n",
            "temp_split [[4, [4, 0, 0]], [4, [3, 1, 0]], [5, [3, 2, 0]]]\n",
            "temp_split [[4, [4, 0, 0]], [4, [3, 1, 0]], [6, [3, 3, 0]]]\n",
            "temp_split [[5, [5, 0, 0]], [4, [3, 1, 0]], [6, [3, 3, 0]]]\n",
            "temp_split [[5, [5, 0, 0]], [5, [3, 2, 0]], [6, [3, 3, 0]]]\n",
            "temp_split [[5, [5, 0, 0]], [5, [3, 2, 0]], [7, [3, 3, 1]]]\n",
            "temp_split [[6, [6, 0, 0]], [5, [3, 2, 0]], [7, [3, 3, 1]]]\n",
            "temp_split [[6, [6, 0, 0]], [6, [3, 3, 0]], [7, [3, 3, 1]]]\n",
            "temp_split [[6, [6, 0, 0]], [6, [3, 3, 0]], [8, [3, 3, 2]]]\n",
            "temp_split [[7, [6, 1, 0]], [6, [3, 3, 0]], [8, [3, 3, 2]]]\n",
            "temp_split [[7, [6, 1, 0]], [7, [3, 3, 1]], [8, [3, 3, 2]]]\n",
            "temp_split [[7, [6, 1, 0]], [7, [3, 3, 1]], [9, [3, 3, 3]]]\n",
            "data [0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1]\n",
            "temp_split [[1, [1, 0, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[2, [1, 1, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[2, [1, 1, 0]], [1, [1, 0, 0]]]\n",
            "temp_split [[2, [1, 1, 0]], [2, [1, 1, 0]]]\n",
            "temp_split [[2, [1, 1, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[3, [2, 1, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[4, [3, 1, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [4, [3, 1, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [5, [4, 1, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[6, [4, 2, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[7, [5, 2, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[8, [5, 3, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[8, [5, 3, 0]], [7, [6, 1, 0]]]\n",
            "temp_split [[8, [5, 3, 0]], [8, [6, 2, 0]]]\n",
            "temp_split [[8, [5, 3, 0]], [9, [6, 2, 1]]]\n",
            "temp_split [[9, [6, 3, 0]], [9, [6, 2, 1]]]\n",
            "temp_split [[10, [6, 4, 0]], [9, [6, 2, 1]]]\n",
            "temp_split [[11, [6, 4, 1]], [9, [6, 2, 1]]]\n",
            "temp_split [[11, [6, 4, 1]], [10, [6, 3, 1]]]\n",
            "temp_split [[11, [6, 4, 1]], [11, [6, 3, 2]]]\n",
            "temp_split [[11, [6, 4, 1]], [12, [6, 3, 3]]]\n",
            "data [0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1]\n",
            "temp_split [[1, [1, 0, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[2, [1, 1, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[3, [2, 1, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[4, [2, 2, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [1, [1, 0, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [2, [2, 0, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [4, [3, 1, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [5, [4, 1, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[6, [4, 2, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[7, [5, 2, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[8, [5, 3, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[9, [6, 3, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[10, [6, 4, 0]], [6, [5, 1, 0]]]\n",
            "temp_split [[11, [6, 4, 1]], [6, [5, 1, 0]]]\n",
            "temp_split [[11, [6, 4, 1]], [7, [6, 1, 0]]]\n",
            "temp_split [[11, [6, 4, 1]], [8, [6, 2, 0]]]\n",
            "temp_split [[11, [6, 4, 1]], [9, [6, 2, 1]]]\n",
            "temp_split [[11, [6, 4, 1]], [10, [6, 3, 1]]]\n",
            "temp_split [[11, [6, 4, 1]], [11, [6, 3, 2]]]\n",
            "temp_split [[11, [6, 4, 1]], [12, [6, 3, 3]]]\n",
            "data [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "temp_split [[1, [1, 0, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[2, [1, 1, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[3, [2, 1, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[4, [2, 2, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[5, [3, 2, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[6, [4, 2, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[7, [5, 2, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[8, [5, 3, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[9, [6, 3, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[10, [7, 3, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[11, [8, 3, 0]], [0, [0, 0, 0]]]\n",
            "temp_split [[11, [8, 3, 0]], [1, [1, 0, 0]]]\n",
            "temp_split [[11, [8, 3, 0]], [2, [2, 0, 0]]]\n",
            "temp_split [[11, [8, 3, 0]], [3, [2, 1, 0]]]\n",
            "temp_split [[11, [8, 3, 0]], [4, [3, 1, 0]]]\n",
            "temp_split [[11, [8, 3, 0]], [5, [3, 2, 0]]]\n",
            "temp_split [[11, [8, 3, 0]], [6, [3, 2, 1]]]\n",
            "temp_split [[11, [8, 3, 0]], [7, [4, 2, 1]]]\n",
            "temp_split [[11, [8, 3, 0]], [8, [4, 3, 1]]]\n",
            "temp_split [[11, [8, 3, 0]], [9, [4, 3, 2]]]\n",
            "temp_split [[11, [8, 3, 0]], [10, [4, 4, 2]]]\n",
            "temp_split [[11, [8, 3, 0]], [11, [4, 4, 3]]]\n",
            "temp_split [[11, [8, 3, 0]], [12, [4, 4, 4]]]\n"
          ]
        }
      ],
      "source": [
        "# load pizza dataframe\n",
        "pizza_data = Arff('pizza.arff')\n",
        "counts = []\n",
        "for i in range(pizza_data.data.shape[1]):\n",
        "    counts += [pizza_data.unique_value_count(i)]\n",
        "\n",
        "# fit and score pizza df\n",
        "clf = DTClassifier(counts=counts)\n",
        "clf.fit_and_score(pizza_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSNiejBECmLy",
        "outputId": "fbc5a652-3c8e-479d-a0a6-28b1d184c83e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[b'young' b'myope' b'no' b'reduced']\n",
            " [b'young' b'myope' b'no' b'normal']\n",
            " [b'young' b'myope' b'yes' b'reduced']\n",
            " [b'young' b'myope' b'yes' b'normal']\n",
            " [b'young' b'hypermetrope' b'no' b'reduced']\n",
            " [b'young' b'hypermetrope' b'no' b'normal']\n",
            " [b'young' b'hypermetrope' b'yes' b'reduced']\n",
            " [b'young' b'hypermetrope' b'yes' b'normal']\n",
            " [b'pre_presbyopic' b'myope' b'no' b'reduced']\n",
            " [b'pre_presbyopic' b'myope' b'no' b'normal']\n",
            " [b'pre_presbyopic' b'myope' b'yes' b'reduced']\n",
            " [b'pre_presbyopic' b'myope' b'yes' b'normal']\n",
            " [b'pre_presbyopic' b'hypermetrope' b'no' b'reduced']\n",
            " [b'pre_presbyopic' b'hypermetrope' b'no' b'normal']\n",
            " [b'pre_presbyopic' b'hypermetrope' b'yes' b'reduced']\n",
            " [b'pre_presbyopic' b'hypermetrope' b'yes' b'normal']\n",
            " [b'presbyopic' b'myope' b'no' b'reduced']\n",
            " [b'presbyopic' b'myope' b'no' b'normal']\n",
            " [b'presbyopic' b'myope' b'yes' b'reduced']\n",
            " [b'presbyopic' b'myope' b'yes' b'normal']\n",
            " [b'presbyopic' b'hypermetrope' b'no' b'reduced']\n",
            " [b'presbyopic' b'hypermetrope' b'no' b'normal']\n",
            " [b'presbyopic' b'hypermetrope' b'yes' b'reduced']\n",
            " [b'presbyopic' b'hypermetrope' b'yes' b'normal']]\n"
          ]
        }
      ],
      "source": [
        "# Load debug training data \n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses.arff --output debug.arff\n",
        "\n",
        "debug_data = arff.loadarff('debug.arff')\n",
        "debug_df = pd.DataFrame(debug_data[0])\n",
        "\n",
        "clf = DTClassifier(target_name='contact_lenses')\n",
        "clf.fit_and_score(debug_df)\n",
        "\n",
        "# Train Decision Tree\n",
        "\n",
        "\n",
        "# Load debug test data\n",
        "\n",
        "\n",
        "# Predict and compute model accuracy\n",
        "\n",
        "\n",
        "# Print the information gain of every split you make.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFOaJxj4CmLz"
      },
      "outputs": [],
      "source": [
        "# Optional/Additional Debugging Dataset - Pizza Homework\n",
        "# pizza_dataset = np.array([[1,2,0],[0,0,0],[0,1,1],[1,1,1],[1,0,0],[1,0,1],[0,2,1],[1,0,0],[0,2,0]])\n",
        "# pizza_labels = np.array([2,0,1,2,1,2,1,1,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGU8PK2xCmLz"
      },
      "source": [
        "## 1.2 Evaluation\n",
        "\n",
        "We will evaluate your model based on its performance on the zoo dataset. \n",
        "\n",
        "Train your model using this dataset: [Evaluation Train Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo.arff)\n",
        "\n",
        "Test your model on this dataset: [Evaluation Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo_test.arff)\n",
        "\n",
        "Parameters:\n",
        "(optional) counts = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2] (You should compute this when you read in the data, before fitting)\n",
        "\n",
        "---\n",
        "Print out your accuracy on the evaluation test dataset.\n",
        "\n",
        "Print out the information gain of every split you make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBr3I6jXCmL0"
      },
      "outputs": [],
      "source": [
        "# Load evaluation training data\n",
        "\n",
        "\n",
        "# Train Decision Tree\n",
        "\n",
        "\n",
        "# Load evaluation test data\n",
        "\n",
        "\n",
        "# Print out the information gain for every split you make\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVfaMvqHCmL0"
      },
      "source": [
        "## 2. (20%) You will use your ID3 algorithm to induce decision trees for the cars dataset and the voting dataset.  Do not use a stopping criteria, but induce the tree as far as it can go (until classes are pure or there are no more data or attributes to split on).  \n",
        "- Implement and use 10-fold Cross Validation (CV) on each data set to predict how well the models will do on novel data.  \n",
        "- For each dataset, report the training and test classification accuracy for each fold and the average test accuracy. \n",
        "- As a rough sanity check, typical decision tree accuracies for these data sets are: Cars: .90-.95, Vote: .92-.95."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQdySr-HCmL0"
      },
      "source": [
        "## 2.1 Implement 10-fold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDRcKCYjCmL1"
      },
      "outputs": [],
      "source": [
        "# Write a function that implements 10-fold cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrurQtTTCmL1"
      },
      "source": [
        "##  2.2 Cars Dataset\n",
        "- Use this [Cars Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/cars.arff)\n",
        "- Make a table for your K-Fold cross validation accuracies\n",
        "\n",
        "*If you are having trouble using scipy's loadarff function (scipy.io.arff.loadarff), try:*\n",
        "\n",
        "*pip install arff &nbsp;&nbsp;&nbsp;&nbsp;          # Install arff library*\n",
        "\n",
        "*import arff as arf*                   \n",
        "\n",
        "*cars = list(arf.load('cars.arff'))   &nbsp;&nbsp;&nbsp;&nbsp;# Load your downloaded dataset (!curl, etc.)*\n",
        "\n",
        "*df = pd.DataFrame(cars)*  \n",
        "\n",
        "*There may be additional cleaning needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I50wXJeCmL1"
      },
      "outputs": [],
      "source": [
        "# Use 10-fold CV on Cars Dataset\n",
        "\n",
        "# Report Training and Test Classification Accuracies\n",
        "\n",
        "# Report Average Test Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z93wyVhjCmL1"
      },
      "source": [
        "## 2.3 Voting Dataset\n",
        "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)\n",
        "- Note that you will need to support unknown attributes in the voting data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUNtg8LBCmL2"
      },
      "outputs": [],
      "source": [
        "# Used 10-fold CV on Voting Dataset\n",
        "\n",
        "# Report Training and Test Classification Accuracies\n",
        "\n",
        "# Report Average Test Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBzLv0l-CmL2"
      },
      "source": [
        "## 2.4 Discuss Your Results\n",
        "\n",
        "- Summarize your results from both datasets, and discuss what you observed. \n",
        "- A fully expanded tree will often get 100% accuracy on the training set. Why does this happen and in what cases might it not?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLJq3SCcCmL2"
      },
      "source": [
        "Discuss your results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCi8FgyzCmL2"
      },
      "source": [
        "## 3. (15%) For each of the two problems above, summarize in English what the decision tree has learned (i.e. look at the induced tree and describe what rules it has discovered to try to solve each task). \n",
        "- If the tree is very large you can just discuss a few of the more shallow attribute combinations and the most important decisions made high in the tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUO9beTOCmL2"
      },
      "source": [
        "## 3.1 Discuss what the decision tree induced on the cars dataset has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxJsPkQkCmL3"
      },
      "source": [
        "Discussion Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip5zeIIFCmL3"
      },
      "source": [
        "## 3.2 Discuss what the decision tree induced on the voting dataset has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlUsN_SfCmL3"
      },
      "source": [
        "Discussion Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGLflx_FCmL3"
      },
      "source": [
        "## 3.3 How did you handle unknown attributes in the voting problem? Why did you choose this approach? (Do not use the approach of just throwing out data with unknown attributes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YmvT4T7CmL3"
      },
      "source": [
        "Discuss how you handled unknown attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bszs7X95CmL3"
      },
      "source": [
        "## 4.1 (10%) Use SciKit Learn's decision tree on the voting dataset and compare your results. Try different parameters and report what parameters perform the best on the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EptPkscCmL3"
      },
      "source": [
        "### 4.1.1 SK Learn on Voting Dataset\n",
        "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpvOixmjCmL4"
      },
      "outputs": [],
      "source": [
        "# Use SK Learn's Decision Tree to learn the voting dataset\n",
        "\n",
        "# Explore different parameters\n",
        "\n",
        "# Report results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByCPKa5vCmL4"
      },
      "source": [
        "Discuss results & compare to your method's results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taIOEaWCCmL4"
      },
      "source": [
        "## 4.2 (10%) Choose a data set of your choice (not already used in this or previous labs) and use the SK decision tree to learn it. Experiment with different hyper-parameters to try to get the best results possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU2XN8cOCmL4"
      },
      "outputs": [],
      "source": [
        "# Use SciKit Learn's Decision Tree on a new dataset\n",
        "\n",
        "# Experiment with different hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbAEEv1RCmL4"
      },
      "source": [
        "## 5. (5%) Visualize sklearn's decision tree for your chosen data set (using export_graphviz or another tool) and discuss what you find. If your tree is too deep to reasonably fit on one page, show only the first several levels (e.g. top 5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXH8-BD2CmL4"
      },
      "outputs": [],
      "source": [
        "# Include decision tree visualization here\n",
        "\n",
        "# Discuss what the model has learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfAQCjEbCmL4"
      },
      "source": [
        "## 6. (optional 5% extra credit) Implement reduced error pruning to help avoid overfitting.  \n",
        "- You will need to take a validation set out of your training data to do this, while still having a test set to test your final accuracy. \n",
        "- Create a table comparing your decision tree implementation's results on the cars and voting data sets with and without reduced error pruning. \n",
        "- This table should compare:\n",
        "    - a) The # of nodes (including leaf nodes) and tree depth of the final decision trees \n",
        "    - b) The generalization (test set) accuracy. (For the unpruned 10-fold CV models, just use their average values in the table)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSdw7W9fCmL4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab_3_decision_tree.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "bf6171e0b005234b90bf150e7dae75bb4dd3ef3eda0e7c742cc9ee610eeebd82"
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit ('.venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
