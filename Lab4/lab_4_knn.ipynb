{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# K-Nearest Neighbor Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "from scipy.io import arff\n",
        "from scipy.spatial import distance\n",
        "import math\n",
        "from scipy.spatial import distance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. (40%) Correctly implement the k-nearest neighbor (KNN) algorithm and the KNN regression algorithm\n",
        "\n",
        "### Code requirements\n",
        "- Use Euclidean distance to decide closest neighbors. \n",
        "- Include optional distance weighting for both algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_a2KSZ_7AN0G"
      },
      "outputs": [],
      "source": [
        "class KNNClassifier(BaseEstimator,ClassifierMixin):\n",
        "    def __init__(self, columntype=[], weight_type='inverse_distance', k_val=3): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            columntype for each column tells you if continues[real] or if nominal[categoritcal].\n",
        "            weight_type: inverse_distance voting or if non distance weighting. Options = [\"no_weight\",\"inverse_distance\"]\n",
        "        \"\"\"\n",
        "        self.columntype = columntype #Note This won't be needed until part 5\n",
        "        self.weight_type = weight_type\n",
        "        self.k_val = k_val\n",
        "\n",
        "    def fit(self, data, labels):\n",
        "        \"\"\" Fit the data; run the algorithm (for this lab really just saves the data :D)\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 2D numpy array with the training targets\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        self.full_data = []\n",
        "        for i in range(len(data)):\n",
        "            self.full_data.append([data[i], labels[i]])\n",
        "        return self\n",
        "    \n",
        "    def predict(self, data):\n",
        "        \"\"\" Predict all classes for a dataset X\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        preds = []\n",
        "        for i in range(len(data)):\n",
        "            k_nearest = [[math.inf, 0] for x in range(self.k_val)]\n",
        "            dist_arr = []\n",
        "            for j in range(len(self.full_data)):\n",
        "                vals = self.full_data[j][0]\n",
        "                dist = distance.euclidean(vals, data[i])\n",
        "                dist_arr.append(dist)\n",
        "                for k in range(self.k_val):\n",
        "                    if dist < k_nearest[k][0]:\n",
        "                        k_near_copy = k_nearest.copy()\n",
        "                        for indx in range(k, self.k_val-1):\n",
        "                            old_k = k_near_copy[indx]\n",
        "                            k_nearest[indx+1] = old_k\n",
        "                        new_k = [dist, self.full_data[j][1]]\n",
        "                        k_nearest[k] = new_k\n",
        "                        break\n",
        "            preds.append(self.predict_output(k_nearest))\n",
        "        return preds\n",
        "\n",
        "    def predict_output(self, k_near):\n",
        "        votes = {}\n",
        "        for i in range(len(k_near)):\n",
        "            label = k_near[i][1]\n",
        "            dist =  k_near[i][0]\n",
        "            if label in votes:\n",
        "                if self.weight_type == 'inverse_distance':\n",
        "                    inv_dist = 1/dist**2\n",
        "                    votes[label] += inv_dist\n",
        "                else:\n",
        "                    votes[label] +=1\n",
        "            else:\n",
        "                if self.weight_type == 'inverse_distance':\n",
        "                    inv_dist = 1/dist**2\n",
        "                    votes[label] = inv_dist\n",
        "                else:\n",
        "                    votes[label] = 1\n",
        "        max_val = 0\n",
        "        label = ''\n",
        "        for key, value in votes.items():\n",
        "            if value > max_val or max_val == 0:\n",
        "                label = key\n",
        "                max_val = value\n",
        "            # tie\n",
        "            elif value == max_val:\n",
        "                label = min(key, label)\n",
        "                max_val = value = votes[label]\n",
        "\n",
        "        return label\n",
        "\n",
        "    #Returns the Mean score given input data and labels\n",
        "    def score(self, X, y):\n",
        "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 2D numpy array with targets\n",
        "        Returns:\n",
        "            score : float\n",
        "                Mean accuracy of self.predict(X) wrt. y.\n",
        "        \"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        correct = 0\n",
        "        for i, pred in enumerate(predictions):\n",
        "            if pred == y[i]:\n",
        "                correct +=1\n",
        "        return correct/len(X)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEDzK14Wpj8H"
      },
      "source": [
        "## 1.1 Debug and Evaluation\n",
        "\n",
        "Debug and Evaluate your model using the parameters below:\n",
        "\n",
        "- Use distance weighting\n",
        "- KNN = 3 (three nearest neighbors)\n",
        "- Don’t normalize the data\n",
        "- Use Euclidean Distance\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1.1 Debug\n",
        "\n",
        "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_test.arff)\n",
        "- Use distance weighting\n",
        "- KNN = 3 (three nearest neighbors)\n",
        "- Don’t normalize the data\n",
        "- Use Euclidean Distance\n",
        "\n",
        "Expected Results:\n",
        "- Not using inverse weighted distancing = roughly [68.29%]\n",
        "- Link to [debug solution](https://github.com/cs472ta/CS472/blob/master/debug_solutions/glass_no_inv_predictions.txt)\n",
        "\n",
        "- Using inverse weighted distancing = roughly [74.39%]\n",
        "- Link to [debug solution](https://github.com/cs472ta/CS472/blob/master/debug_solutions/glass_inv_predictions.txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convertBytestoString(df):\n",
        "  for col in df:\n",
        "    if isinstance(df[col][0], bytes):\n",
        "      df[col] = df[col].str.decode(\"utf8\")\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F6CAyEJopj8I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc not using weighted distancing:  0.6829268292682927\n",
            "acc using weighted distancing:  0.7439024390243902\n"
          ]
        }
      ],
      "source": [
        "# Load glass data\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_train.arff --output debug.arff\n",
        "# Train on training set\n",
        "data = arff.loadarff('debug.arff')\n",
        "debug_df = convertBytestoString(pd.DataFrame(data[0]))\n",
        "debug_np = np.array(debug_df)\n",
        "clf = KNNClassifier(weight_type='no_weight')\n",
        "train = np.array(debug_np[:,0:-1])\n",
        "targets = np.array(debug_np[:,-1])\n",
        "res = clf.fit(train, targets)\n",
        "\n",
        "\n",
        "# Predict on test set\n",
        "!curl -s https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_test.arff --output debug_test.arff\n",
        "debug_test = arff.loadarff('debug_test.arff')\n",
        "debug_test_df = convertBytestoString(pd.DataFrame(debug_test[0]))\n",
        "debug_test_np = np.array(debug_test_df)\n",
        "test_labels = debug_test_np[:,0:-1]\n",
        "test_targets = debug_test_np[:,-1]\n",
        "acc = res.score(test_labels, test_targets)\n",
        "print('acc not using weighted distancing: ', acc)\n",
        "\n",
        "# using inverse weighted distancing\n",
        "clf = KNNClassifier(weight_type='inverse_distance')\n",
        "iw_res = clf.fit(train, targets)\n",
        "iw_acc = iw_res.score(test_labels, test_targets)\n",
        "print('acc using weighted distancing: ', iw_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxbxlFzjpj8I"
      },
      "source": [
        "### 1.1.2 Evaluate\n",
        "\n",
        "We will evaluate your model based on its performance on the [diabetes](https://archive.ics.uci.edu/ml/datasets/Diabetes) problem.\n",
        "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_test.arff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZydqKufipj8J"
      },
      "outputs": [],
      "source": [
        "# Load diabetes data\n",
        "\n",
        "# Train on training set\n",
        "\n",
        "# Predict on test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. (10%) Use the k-nearest neighbor algorithm (without distance weighting) for the [magic telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope) problem\n",
        "\n",
        "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff) \n",
        "\n",
        "### 2.1\n",
        "- Try it with k=3 and without normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "outputs": [],
      "source": [
        "# Load magic telescope data\n",
        "\n",
        "# Train/Predict without normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDankjHLpj8L"
      },
      "source": [
        "### 2.2\n",
        "- Try it with k=3 and with normalization (input features normalized between 0 and 1). Use the normalization formula (x-xmin)/(xmax-xmin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAb2ot8qpj8L"
      },
      "outputs": [],
      "source": [
        "# Train/Predict with normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI0BWb9epj8M"
      },
      "source": [
        "*Discuss the accuracy results of using normalized data vs. unnormalized data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_yormHgpj8N"
      },
      "source": [
        "### 2.3\n",
        "\n",
        "- Using your normalized data, create one graph with classification accuracy on the test set over k values. \n",
        "    - Use odd values of k from 1 to 15.\n",
        "- As a rough sanity check, typical knn accuracies for the magic telescope data set are 75-85%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3PBKAmepj8N"
      },
      "outputs": [],
      "source": [
        "# Train/Predict with normalization using k=1,3,...,15\n",
        "\n",
        "# Graph classification accuracy over k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcuWW3gWpj8O"
      },
      "source": [
        "# For the rest of the experiments use only normalized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "## 3. (10%) Use the regression variation of your algorithm (without distance weighting) for the [housing price prediction](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) problem.\n",
        "\n",
        "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_test.arff).\n",
        "- Use Mean Square Error (MSE) on the test set as your accuracy metric for this case.\n",
        "    - Do not normalize regression output values\n",
        "- Graph MSE on the test set with odd values of k from 1 to 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBGUn43ASiXW"
      },
      "outputs": [],
      "source": [
        "# Load housing price prediction data\n",
        "\n",
        "# Train/Predict using k=1,3,...,15\n",
        "\n",
        "# Graph MSE over k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v19fpixqTe-7"
      },
      "source": [
        "## 4. (15%) Repeat your experiments for magic telescope and housing using distance-weighted (inverse of distance squared) voting and discuss your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkuX9RPEpj8P"
      },
      "source": [
        "## 4.1 Magic Telescope Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCPFUAGTS2sX"
      },
      "outputs": [],
      "source": [
        "# Train/Predict magic telescope using distance-weighted voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APz2NlNepj8P"
      },
      "source": [
        "## 4.2 Housing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGxzfVvNpj8P"
      },
      "outputs": [],
      "source": [
        "# Train/Predict housing using distance-weighted voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoOLKUSnpj8Q"
      },
      "source": [
        "*Discuss your results*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhFe-C_ipj8Q"
      },
      "source": [
        "## 5. (10%) Use the k-nearest neighbor algorithm to solve the [credit-approval](https://archive.ics.uci.edu/ml/datasets/Credit+Approval) (credit-a) problem.\n",
        "\n",
        "- Use this [dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/credit_approval.arff)\n",
        "    - Use a 70/30 split of the data for the training/test set\n",
        "- Note that this set has both continuous and nominal attributes, together with don’t know values. \n",
        "- Implement and justify a distance metric which supports continuous, nominal, and don’t know attribute values\n",
        "    - You need to handle don't knows with the distance metric, not by imputing a value.\n",
        "    - More information on distance metrics can be found [here](https://www.jair.org/index.php/jair/article/view/10182/24168).\n",
        "- Use your own choice for k.\n",
        "- As a rough sanity check, typical knn accuracies for the credit data set are 70-80%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkz2k5Dzpj8Q"
      },
      "outputs": [],
      "source": [
        "# Load dataset and split into train/test sets\n",
        "\n",
        "# Train/Predict credit-approval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOIBMx8Rpj8Q"
      },
      "source": [
        "*Explain and justify your distance metric*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 6. (15%) Use the scikit's KNN Classifier on magic telescope and KNN Regressor on housing and compare your results.\n",
        "\n",
        "- Try out different hyperparameters to see how well you can do. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "outputs": [],
      "source": [
        "# Train/Predict magic telescope using scikit's KNN\n",
        "\n",
        "# Train/Predict housing using scikit's KNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Report your comparison*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 7. (optional 5% extra credit): For the best value of k for any one of the datasets, implement a reduction algorithm that removes data points in some rational way such that performance does not drop too drastically on the test set given the reduced training set.\n",
        "\n",
        "- Compare your performance on the test set for the reduced and non-reduced versions and give the number (and percentage) of training examples removed from the original training set. How well does your reduction algorithm work?\n",
        "    - Note that performance for magic telescope is classification accuracy and for housing it is mean squared error.\n",
        "    - Magic Telescope has about 12,000 instances and if you use a leave one out style of testing for your data set reduction, then your algorithm will run slow since that is n2 at each step.\n",
        "    - If you wish, you may use a random subset of 2,000 of the magic telescope instances.\n",
        "    - More information on reduction techniques can be found [here](http://axon.cs.byu.edu/~martinez/classes/478/slides/IBL.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iY77P7gk1Nh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab_4_knn",
      "provenance": []
    },
    "interpreter": {
      "hash": "8714eab89ae485900fa73f399cda1640b5bb7d68dc82ec2a5597c91e33d54393"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('.venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
